---
title: 构建可伸缩的平台工程软件栈架构
date: 2023-05-09 22:08:49
tags: 
- multi-runtime
- platform engineering
- multi-cluster
- multi-cloud
categories:
- Software Engineering
---

## 平台工程是新概念吗？

[平台工程（Platform Engineering）](https://platformengineering.org/blog/what-is-platform-engineering)，是通过为企业内部研发人员提供一整套工具链和资源以及知识，来更好的为研发人员构建工作流，以使研发团队在 DevOps 实践上更易于达成 [DORA Metrics](https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance) 所描绘的高效能团队要求。

平台工程在企业内部一般是通过 IDP 内部开发者平台（Internal Developer Platforms）来落地实现。通过 IDP，研发人员能够方便灵活的对业务应用进行研发和交付，而不必将大量时间花费在学习和使用交付工作流所依赖的各种底层设施。而企业通过构建小规模的[平台工程团队](https://www.thoughtworks.com/zh-cn/radar/techniques/platform-engineering-product-teams)，开发并维护内部开发者平台，就能服务数倍于该团队人数的业务研发团队。

实际上，很多人在了解到平台工程这一概念后，都会心生疑惑：“平台工程是个新概念吗，听着耳熟啊？”，“这不就是我们公司的 xxx 平台吗？ （这里的 xxx 可能是 **研发管理平台** / **DevOps 平台** / **开发者平台** / **一站式运维平台** 等等”。

的确，至少在笔者所经历过的雇主、客户等诸多场景中，就接触过（甚至咨询过）不下 5 个类似的平台，虽然它们的名称、内部编号各不相同，但全都至少会提供包括代码仓、CI/CD 流水线、配置中心、制品库、资产管理、APM、日志监控、链路追踪等基础能力。通过这些能力，业务研发团队可以相对简单和方便的将业务交付到生产。

### 真实企业的 DevOps

参考 2018 年由信通院组织起草的[《研发运营一体化（DevOps）能力成熟度模型》](https://hbba.sacinfo.org.cn/stdDetail/12bebc74402d9eced9cc0440ef6e4f879c11718e5df86101f985255fb24c35e0)，定义了：

> 研发运营一体化是指在 IT软件及相关服务的研发及交付过程中，将应用的需求、开发、测试、部署和运营统一起来，基于整个组织的协作和应用架构的优化，实现敏捷开发、持续交付和应用运营的无缝集成。

并且基于此进一步描述了 DevOps 在敏捷开发管理、持续交付和技术运营几个子项下对企业能力的要求。并通过一个成熟度模型来评估不同企业 DevOps 能力所处的阶段。

{% asset_img YDT-3763.png %}

显然 DevOps 能力的建设并不是只要通过几个工具组合起来就能落地的实践，而是一整套涉及管理、流程、工具和文化的体系化工程和变革。

许多企业没有精力和人才来构建整个 DevOps 体系，就只好先从便宜且收效快的地方入手：找原来的运维团队抽调几个人，采用开源方案搭建内部的代码仓（如 GitLab）和流水线（如 Jenkins），再手写一些通用的脚本来实现基础的制品部署。能有更多投入的企业，还会建设一些需求管理工具、资产管理工具等（对接云服务提供商的 API）。

<img src="https://kubesphere.io/images/devops/dev-ops.png" style="zoom: 67%;" />

但在业务开发团队的视角下，这些与 DevOps 相关的工作似乎还是 Tech Lead 或者专职 DevOps（是的，真有这一岗位）的职责。究其原因仍然是普通开发人员玩不转或者根本不想考虑代码到底如何才能交付上线。

上述实际情况大量存在于各类企业，这与 “You build it, you run it” 的 DevOps 精神是完全相悖的。此外由于与 DevOps 相关的工作通产会被代理给更资深的人员，因此他们的时间也被明显的侵占了。而随着云原生的不断发展，各类新的概念和实践被提出，研发人员愈发的感觉到知识追赶的压力，也就越来越抵触做 DevOps 相关的事情。

{% asset_img cncf-landscape.png %}

真正具备了高 DevOps 成熟度的企业，有如下两种不同的实践：

1. 团队自组织的 DevOps 实践：完全开放的独立式团队，由成熟的研发人员组成，践行敏捷文化，根据实际情况自建或选择工具链（如 Github Actions），通过 IaC 确保流程和基础设施全部代码化管理，自主选择构建配置管理、Key 管理、可观测性等能力的方案。
2. 企业统一构建成熟的内部开发者平台：通过在企业内部搭建非常完善的平台、工具甚至基础设施，推行全公司一致的研发流程和团队管理实践，并通过构建复杂的指标体系来评估不同的研发团队并与绩效挂钩，自上而下的推动 DevOps 能力的提升。

上述两种不同的实践中，第一类更适合分散作战的小型精英团队，能快速且成本可控的交付软件。而对于第二类实践，其建设投入非常可观，更适合大型企业。事实上很多大型企业自建的内部开发者平台，已经和平台工程的概念内容逐步趋同了。



## 平台工程能力的演进

前面提到，许多企业都可能期望通过构建一定的工具和平台来弥补其 DevOps 能力的欠缺，不同的投入水平会得到不同的效果。而大型企业在经过持续的投入和演进后，也已经构建了趋近于平台工程概念所描绘的 IDP。

在 CNCF App Delivery TAG 发布的 [*平台工程白皮书 Platforms White Paper*](https://tag-app-delivery.cncf.io/whitepapers/platforms/) 中，对企业平台工程能力的成熟度做了总结（成熟度由低到高）：

>1. 产品开发人员可以根据需要配置相关能力，并立即使用这些能力来运行系统，如计算、存储、数据库或身份认证。
>2. 产品开发人员可以根据需要配置服务空间，并在服务空间中运行流水线和任务，来保存制品和配置，以及收集遥测数据。
>3. 第三方软件的管理员可以按需配置依赖，如数据库等，并轻松安装和运行该依赖软件。
>4. 产品开发人员可以通过模板（templates）配置完整的环境，这些模板结合了专用场景（如web开发或MLOps）下所需的运行时（run-time）和开发时（development-time）服务。
>5. 产品开发人员和管理人员可以通过自动数据采集和标准看板来观测已部署服务的功能、性能和成本。

可以看出，按成熟度模型的划分，IDP 所能提供的能力从基础到高级可归纳为*「基础设施 -> CICD -> 自助 PaaS -> 模板自动化 -> 可观测」* 当然，以上的一切都是云原生的。

基于以上能力，白皮书描绘了平台工程的能力图谱，以及其所处的位置，即处于产品应用团队与能力服务提供者之间：

<img src="https://tag-app-delivery.cncf.io/whitepapers/platforms/assets/platform_components.png" style="zoom: 67%;" />

#### 企业诉求的变化

企业在不同的规模和发展阶段下，对平台工程所能提供的能力，以及 IDP 建设的成本的诉求，可能是大相径庭的。

纵然在这篇 [*何为平台工程*](https://platformengineering.org/blog/what-is-platform-engineering) 中提到，只要组织规模达到 20~30 人，就可以开始关注 IDP 了。但小规模企业对平台工程能力建设的主要考量点可能还是**基本可用**和**成本最低**。通过尽量使用便宜的甚至免费的服务来满足 DevOps 的要求就是最好的选择，因此小规模企业可能更容易接受 SaaS 化的解决方案，例如云提供商打包出售的开发者服务，或者类似 [Github Pro](https://github.com/pricing)、[Coding](https://coding.net/) 等方案。

而当企业达到一定规模，并且处于高速发展期时，其诉求可能变化为更关注软件交付的速度，以及 IDP 的快速扩展能力。毕竟高速发展的业务需要平台成支撑其快速试错、快速占领市场的目标，同时大量扩张的业务和用户量也要求 IDP 能更快、更简单的扩展新功能以支持业务发展。

大规模企业在平台工程能力的建设上，就更倾向于要求易用、合规、稳定、并且能降本增效。为了支撑大型企业中各式各样的业务需求，IDP 在用户交互上需要简明易用，降低学习门槛。而由于存在跨国跨地区的业务，以及内部用户众多，合规性和稳定性的要求也非常重要，IDP 可以没有存在感，但不能因为 IDP 而导致企业发生损失。最后，大规模企业流程多、效率差，降本增效是持续的主题，IDP 需要尽可能的自助化、傻瓜化从而降低人员使用成本，提升效率。

因此，企业对平台工程能力建设的诉求，在不同规模下不同，在不同时期下也不同。那么在设计 IDP 的整体架构时，如何才能满足不同的诉求呢？采用可伸缩、模块堆叠的软件栈架构是一种可行的办法。



## 可扩展的软件栈架构

软件栈（或称[解决方案栈](https://en.wikipedia.org/wiki/Solution_stack)）是通过一组软件子系统或组件来构建的一个完整平台，在该平台之上可以运行特定的应用程序。这种基于抽象分层的栈模式在软件系统中十分常见。

一个简单的 Web 服务就可以是一种软件栈，其包含的组件有 Web 服务器、数据库、容器、文件系统、网络栈、硬件驱动等，它们分别处于不同的抽象层次，由不同的人开发和维护，通过标准化接口进行通信和交互。

对于前面所提到的构建平台工程能力过程中，将企业需要自研的部分尽可能收敛和内聚，通过设计标准抽象层，并尽可能借助开源社区的力量，构建分层的、可扩展的、组件化的软件栈架构，就能够充分应对平台工程能力建设在成本和功能上多变的场景化诉求。

基于上述思想构建的可扩展的平台工程架构，如下图所示：



可见，设计自上而下的抽象层，在层内组合多种组件，就可以搭建起灵活的平台工程架构。

### 可扩展的能力抽象层

作为平台架构的最上层，能力抽象层的目标是通过提供合理的抽象与接口，让应用在开发态和运行态都能更关注于业务本身，而将运维能力侧和公共组件侧的需求尽可能代理出去。

#### 统一应用模型

通常意义上，应用在 Day2 阶段的持续时间要远大于 Day0 和 Day1，这也就导致了交付和运维的工作是繁杂和冗长的。软件部署早已不是把包丢到服务器上然后启动进程就完事，还包括配置管理、服务拓扑、副本扩缩、流量分发、监控、审计、成本优化等等各种要求。

为了满足这些要求，需要通过各种工具和手段来实现，而正如前文提到的，很多开发者可能并不想去了解这些玩意儿到底如何使用。毕竟作为应用开发者，他们更清楚自己软件的入口是`/index`，而不清楚从主域名经过几层 Nginx 才能路由到到`/index`。

通过定义一套标准的应用交付模型，就可以将应用开发团队和平台团队的关注点进行分离，由开发者定义特定应用在交付和运行中的需求，由平台团队来实现这些需求。

[OAM（Open Application Model](https://github.com/oam-dev/spec/blob/master/README.md)就是这样的一种模型标准。

<img src="https://github.com/oam-dev/spec/blob/master/assets/overview.png?raw=true" style="zoom: 50%;" />

平台团队提供 ComponentDefinition 来描述不同应用的部署模型，如`通过 K8s Deployment 部署的无状态后端服务`，`直接推送 CDN 的静态前端页面` 等。应用开发者只需挑选某个 Component 来描述应用，并设置一些属性参数，如镜像名、ENV、端口号等等即可。

同时，平台团队还提供了对 Traits 和 Scopes 的定义，这允许开发者为他们的应用添加运维特征如动态扩缩，灰度发布，负载均衡等，以及分组特征如安全组，AZ等。

因此，开发者只需要将应用模型以类似 `yaml` 的形式维护在代码仓内，随着 [GitOps](https://www.weave.works/blog/what-is-gitops-really) 流程，应用就会顺滑的交付上线。

> [KubeVela]([kubevela.io](https://kubevela.io/)) 实现并扩展了 OAM

#### 分布式能力抽象

在成规模的服务化体系中，应用可能依赖了越来越多的中间件以及三方服务，它们提供了应用实现业务目标所需要的各种分布式能力。然而，传统的 SDK 集成方式让这些分布式能力变成了一个个的孤岛，难以统一治理，导致维护困难、升级困难，降低了整体的研发效率。

平台可以将各种分布式能力进行归类和抽象，为应用提供统一的分布式能力抽象层，因而应用只需要通过抽象层调用标准化能力，由平台团队维护实际的能力实现组件。

[多运行时架构](https://www.lenshood.dev/2022/12/06/multi-runtime/)就是基于上述思想提出的解决方案：

<img src="https://www.lenshood.dev/2022/12/06/multi-runtime/3.webp" style="zoom:67%;" />

多运行时架构的理念，是将各种分布式能力归纳为 4 个部分：生命周期、网络、状态以及绑定。传统场景下，这四大能力是由各类基础设施和中间件来提供的。

<img src="https://www.lenshood.dev/2022/12/06/multi-runtime/7.png" style="zoom:50%;" />

通过`Mircologic + Mecha`，即微业务与所谓“机甲”相组合的方式，将业务对分布式能力的需求全部交给 Mecha 运行时来代理，而真实提供分布式能力的组件，通过 Mecha 与业务应用隔离。

平台通过为每一个业务应用提供 Mecha 运行时，隔离需求与实现，因此能够方便的对各种中间件进行维护和扩展。

> 多运行时架构的实现方案有 [Dapr](https://dapr.io/)、[Layotto](https://mosn.io/layotto/#/zh/README) 等



###可扩展的 DevOps 组件层

#### CI/CD

DevOps 组件层所提供的最基本的能力应该就是 CI/CD Pipeline 了。通过 Pipeline 来拉取 Repo 中的代码，并一步步的执行编译、检查、测试、部署、发布，这就是所有企业尝试 DevOps 工具的第一步。

<img src="https://images.contentstack.io/v3/assets/blt300387d93dabf50e/blt5011b7706b99630a/6388867529632c37569f030f/Guide-To-GitOps-Diagrams4.png" style="zoom:50%;" />

CI/CD Pipeline 的运行，本质上是执行了一个 DAG，各个阶段具体做的事情只是挂载在 DAG 节点上的细节。因此大多数的 CI/CD 工具实际上都是一个 Workflow Engine。

上一节提到的实现了 OAM 的 KubeVela，就基于 OAM 模型扩展了 Workflow 的定义，通过 Workflow 扩展定义，可以将 Pipeline 的部分直接集成在 OAM 模型当中。

<img src="https://camo.githubusercontent.com/3c9f24e500b84bc31c744c623573b11e00247f49364fe7bc673e03faa56ee631/68747470733a2f2f7374617469632e6b75626576656c612e6e65742f696d616765732f312e362f776f726b666c6f772d617263682e706e67" style="zoom: 50%;" />

KubeVela 自身实际上就是一个 CD 系统，并通过[workflow 插件](https://github.com/kubevela/workflow/tree/main)支持了部分预定义步骤的执行（如编译镜像等）。此外，KubeVela 还支持在 Workflow 定义的各个 Pipeline Step 中触发外部的 CI 系统，这就实现了方便扩展能力（KubeVela 称这一设计为 [Unified Declarative CI/CD](https://kubevela.io/docs/tutorials/s2i)）。

#### 资源管理 + IaC

在实际当中，除了计算、存储、网络三大基础资源外，业务应用还会依赖大量由 CSP 提供的 PaaS 中间件服务（如数据库、消息服务、负载均衡等）。虽然通过前文介绍的多运行时架构，应用开发者能够不必关心具体中间件的实现和维护，但对于平台团队而言，大多数 PaaS 服务都是非 K8s 的，通常企业会通过维护自己的 Terraform 模块库来实现对资源的自动化管理，但想要将它们纳入 K8s 下统一管理仍需要可观的人力付出，尤其是在跨云场景下更加复杂。

[Crossplane](https://www.crossplane.io/) 正是为了解决这一问题而诞生。

<img src="https://docs.crossplane.io/media/composition-how-it-works.svg" style="zoom: 50%;" />

Crossplane 通过 Provider 实现对具体 PaaS 资源的操作，在其[官方市场](https://marketplace.upbound.io/)中已经有数十家 CSP 开发的 Providers。Crossplane 允许用户自己定义资源，并通过标准的控制器模式来完成对资源的管理。因此，通过 Crossplane 可以声明式的管理 PaaS 资源。实际的资源申请场景中，Crossplane 借鉴了 K8s PV 与 PVC 的概念，资源提供方通过创建资源定义，来发布可用的资源，而资源使用方通过构建 Claim 来发出对资源的请求。最后，通过 Crossplane 控制器就能完成这一需求匹配过程。

因此，应用的资源配置可以直接以 Claim 的形式在代码仓中维护，随着 GitOps 配置和变更资源。

#### 可观测性

常见的可观测性能力通过 Metrics、Logs 以及 Tracing 来分别采集系统的指标、日志和链路追踪。

在 [OpenTelemetry](https://opentelemetry.io/) 出现以前，上述三种不同种类的观测数据各自存在特定的探针、数据格式以及标准，从而导致后端系统的选择绑定了几种方案而难以替换和扩展。

OpenTelemetry（简称 otel） 作为可观测性系统前后端之间的抽象层，整合了一套标准数据模型，使得数据探针和数据处理系统不在相互依赖。

<img src="https://opentelemetry.io/img/otel_diagram.png" style="zoom:67%;" />

除了各种开源方案对 otel 的支持外，包括 AWS、Azure 和 GCP 在内的许多 CSP 都在其产品内支持了 otel 标准。因此引入 otel 能够极大的增强在可观测性能力上的扩展性。

#### 安全

云原生安全的范畴很广，从研发态到运行态包括但不限于代码扫描、漏洞检测、配置检查、权限控制、镜像安全、容器安全、密钥管理等等，CNCF [云原生安全白皮书](https://github.com/cncf/tag-security/blob/main/security-whitepaper/v1/cloud-native-security-whitepaper-simplified-chinese.md)对此在概念上有详细的讨论。

为了确保应用的全生命周期安全，平台团队可以通过多种实践（可参考 [DevSecOps](https://www.thoughtworks.com/insights/decoder/d/devsecops)）来为应用开发者提供全面的、低侵入的安全保障。

![](https://wac-cdn.atlassian.com/dam/jcr:5f26d67b-bed6-4be1-912b-4032de4d06b0/devsecops-diagram.png?cdnVersion=1031)

许多安全类实践都可以与 Pipeline 相结合，从代码提交开始，可以配置代码扫描工具对变更代码和引用的库进行安全和漏洞扫描，生成镜像后也可以对镜像进行扫描，此外包括配置变更以及由 OAM 实际生成的 Helm Chart 也可通过工具进行安全扫描。Pipeline 上的各种工具扫描是非常适合引入的安全实践。

当应用被发布以后，守护运行态安全更是至关重要。基本的 IAM 权限体系是必要的，通过最小权限的原则尽可能限制应用的操作面。通过为系统中每一个实体设置证书来构建零信任网络，增强系统的安全可信能力，同时对证书的管理和分发也很关键。通过安全容器，能够防止恶意代码突破容器的隔离限制而越权控制主机。基于独立的加密存储库，保存各类密钥和密码，降低泄露风险。



### 可扩展的资源编排层

据 [CNCF 2022 Annual Survey](https://www.cncf.io/reports/cncf-annual-survey-2022/) 中的数据，CNCF End User 所在组织对 K8s 的采纳率已经达到 89%（其中 64% 已经应用在生产环境），K8s 作为云原生事实标准的地位愈发稳固。K8s 通过优秀的资源抽象能力，屏蔽了各类 CSP 在计算存储网络三大资源上的差异化。因此，包括应用及前文描述的各类 DevOps 组件，都建议运行在以 K8s 生态为基础的容器资源上。平台有义务为上层提供灵活的、开箱即用的、标准化的生命周期管理与资源编排能力，以支持各类应用和服务的运行。

根据 [CNCF 对云原生的定义](https://github.com/cncf/toc/blob/main/DEFINITION.md)，云原生应用应该能在云上自由的弹性扩展。从这一点看，理想的云原生基础设施，应该能为应用提供无限的资源和难以察觉的扩缩速度，这当然不现实。不过，企业在业务发展过程中确实存在很多场景（例如隔离性、可用性、合规性或使用成本等），需要将应用运行在正确的位置上，并且支持位置的动态调整。对于这些需求，企业可以采用基于多集群技术而构建的可扩展资源编排解决方案。

笔者在文章 [*理解 K8s 多集群*](https://www.lenshood.dev/2023/03/09/k8s-multi-cluster-1/) 中，描述了实现多集群管理的核心要素，为了支撑可扩展资源编排，如下列举几个设计要点：

#### 跨集群动态调度

多集群本质上是为了让应用运行在更合适的位置。为了高可用、成本控制、合规性等目的，业务应用可能会被动态调度到不同的 K8s 集群上。实施调度的关键是调度策略。

通过调度策略，我们期望能解决 ”什么样的应用” 需要被调度到 “哪类集群” 的问题。显然，应用有其自身独特的属性集，集群也一样。从属性集的角度看，调度策略问题就可以转化为应用与集群属性集之间的最优匹配问题。

<img src="https://www.lenshood.dev/2023/03/09/k8s-multi-cluster-1/sched-chain.jpg" style="zoom:50%;" />

应用的属性集可能包括：命名空间、资源依赖、副本数、镜像名、租户归属、应用亲和性/反亲和性、最小资源需求等等，集群的属性集可能包括 AZ、地区（Region）、节点数、已分配 Pod 数、资源总量/余量、污点（Taint）等等。

进行调度决策时，待调度应用与待选集群的属性集依次通过所有过滤型决策器和打分型决策器，最终找到一个（或一组，考虑多副本高可用）分数最高的集群，调度完成。而下达调度决策的前提，是多集群控制面能准确的获悉集群中的各种状态，因此状态数据的收集也至关重要。

#### 应用模型扩展

为了满足多集群管理的需求，传统的单集群应用模型需要进行扩展，以允许应用开发者能定义应用的部署偏好，从而更符合实际的业务目标。

对应用模型的扩展可以分为规格扩展和状态扩展。

对于规格扩展，又可分为两类：限制（constrains）和提示（hints）：

- 限制（constrains）：代表了应用对跨集群管理的强制性要求，如亲和性/反亲和性，最小副本数，污点容忍性（Taints Tolerations）等等
- 提示（hints）：代表了对多集群管理决策与动作的非强制性提示，如优先级，副本分配偏好，资源需求等等

而状态扩展主要扩展的是应用在多个集群上的状态。这包括应用实际在每个集群上的副本数，运行健康状况，曾经被调度的历史等等。状态扩展恰恰是为了将收集到的状态数据聚合在应用模型上，以便于调度器的工作。

根据前文描述的 OAM 模型，平台团队可以针对限制和提示定义多种 Traits，以供应用开发者选用。

#### 集群标准化

基于公版 K8s 扩展和改造的 K8s 发行版层出不穷，这包括各类 CSP 提供的云托管服务（如 EKS），也包括许多开源的方案（如 K3s），发行版 K8s 本质上是在不同场景下提供更合适的容器平台方案。

K8s 社区一方面鼓励定制版的研发以提升社区生态的健康发展，另一方面，CNCF 提出了 [K8s一致性认证](https://www.cncf.io/certification/software-conformance/) 用来确保所有发行版在 API 层面与开源公版兼容，从而在一定程度上达成约束：可以相对安全的在通过认证的发行版之间切换。

但即使是一致性认证的存在，企业在选择 K8s 时也仍然可能面临多集群之间版本一致性、版本升级的困难。

[Gardener](https://gardener.cloud/) 是 SAP 开源的 K8s 多集群解决方案，在设计概念上，Gardener 期望作为一种管理大量 K8s 集群的组件，借助 Gardner，用户能够实现在接入各种不同类型底层基础设施的同时，方便的在其上构建标准的 K8s 集群。

![](https://gardener.cloud/__resources/gardener-architecture-overview_2bd462.png)

Gardener 除了能在各种差异化基础设施上管理 K8s 集群的生命周期，还能够确保在这些基础设施上运行的 K8s 集群具有完全相同的版本、配置和行为，这能简化应用的多云迁移。Gardener 还提供了[一个页面](https://k8s-testgrid.appspot.com/conformance-all)专门介绍其不同版本的标准化K8s 集群与不同云提供商的兼容情况。

除此之外，许多 2B 企业在销售其产品时，也会遇到客户基础设施中的各种 “魔改” 版本（显然不会通过一致性测试）。针对这类场景下的产品兼容性需求，企业一方面会尽量收敛产品对 K8s API 的依赖，同时也会自研类似代理层模块，来屏蔽五花八门的 “魔改” 版本带来的冲击。



###可扩展的基础设施层

在可扩展的资源编排层中，抽象资源通过 K8s 以标准的 CRI、CSI、CNI 形式提供给上层，这给了基础设施层很大的灵活度和扩展性。基础设施层可以自由的通过多云和混合云等技术来根据实际需要向上提供底层的计算、存储、网络资源。

#### 集群即资源

K8s 是对基础设施层的抽象，因此从整体上看，基础设施层的目标就是向资源编排层交付 K8s 集群。然而不论是基于云虚拟机自建 K8s，还是直接使用 CSP 定制化和代管的集群（如 EKS，GKE 等），在各种 CSP 上构建 K8s 集群都涉及到许多适配性工作。

[ClusterAPI（简称 CAPI）](https://github.com/kubernetes-sigs/cluster-api) 是 K8s “Cluster Lifecycle SIG（集群生命周期特别兴趣小组）” 发起的项目，CAPI 尝试通过定义标准基础设施 API 来统一集群生命周期管理，各类 CSP 自行提供实现了标准 API 的 “Provider” 来支持自动化操作集群资源，由于其官方背景，目前已有[数十种 Provider](https://cluster-api.sigs.k8s.io/reference/providers.html) 可供选择（不仅包含 aws 等公有云，还包含了 OpenStack，OCI 等其他方案）。

<img src="https://cluster-api.sigs.k8s.io/images/management-cluster.svg" alt="CAPI 架构" style="zoom:67%;" />

CAPI 的价值不仅在于对各种 CSP 的全面适配，更重要的是通过它能够实现集群的自动化创建和销毁，也就是实现了**集群即资源**，从极大的缩短了资源编排层的扩展速度。

#### 跨云网络

通常，CSP 会默认提供 VPC 来实现灵活的隔离，而跨云之间更是完全隔离，想要互相通信只能通过专线或公网路由。

在多云架构下或多或少会存在跨云网络连通需求，采用 Overlay 网络的方式，可以在不同的云或 VPC 之间建立虚拟扁平网络，实现直接网络连通。

![](https://www.lenshood.dev/2023/03/09/k8s-multi-cluster-1/network-overlay.jpg)

Overlay 网络的本质是隧道技术，通常是在三层网络上构建隧道传输二层网络包来实现虚拟网络。Overlay 网络的优势在于它构建的虚拟扁平网络让上层通信不再依赖复杂的路由策略，但类似 [VxLan](https://en.wikipedia.org/wiki/Virtual_Extensible_LAN) 的技术，只对网络数据包进行了再封装，并没有任何安全性可言，因此当用于公网间建立隧道的时候会采用加密协议传输，如 [IPSec](https://en.wikipedia.org/wiki/IPsec)，[WireGuard](https://www.wireguard.com/) 等。

> 相关开源组件有 [Kilo](https://kilo.squat.ai/)、[Submariner](https://submariner.io/) 等



## 总结

