---
title: Go Runtime 设计：计算资源调度
mathjax: true
date: 2022-03-10 00:34:44
tags: 
- source
- go
categories:
- Golang
---

## 1. 为什么需要 GoRoutine？

### 1.1 多少个线程是最优的？

我们知道，计算机执行一项任务，通常情况下需要由计算、存储、I/O 等多个组件配合运转才能完成。由于 CPU 与其他设备之间速度的巨大差异，我们更倾向于利用多任务同时运行的方式，来最大化的利用 CPU 的计算能力，这也就是并发编程的意义所在。

而由于多核处理器的广泛普及，在多个 CPU 核心上 ”并行的“ 进行多任务 ”并发“，是编写高效程序的必经之路。从程序执行层次的角度看，”并行“ 更倾向于在底层语境下，指代多个 CPU 核心能同时执行代码片段，而 ”并发“ 更倾向于在高层语境下，指代多个任务能同时在计算机上运行。

OS 通过调度机制帮我们实现了将用户层面的多任务并发，映射到硬件层面的多核心并行。从最大化资源利用的角度讲（暂时抛开任务执行公平性不谈），其映射机制，是对 CPU 资源的一种 “超卖”：任务可能处于执行和等待（包括阻塞）两种状态，执行状态下需要 CPU 资源而等待状态下则可以出让 CPU 资源给其他任务使用。根据任务类型的不同，通常可能分成 CPU 密集型任务与 I/O 密集型任务。

那么理论上，到底要同时执行多少个任务（线程数），才能最大化的利用计算资源呢？《Java 并发编程实战》中给出了如下公式：
$$
N_{threads}=N_{cpu}*U_{cpu}*(1+\frac{W}{C})
\\
\\
N_{threads}=number\ of\ CPUs
\\
U_{cpu}=target\ CPU\ utilization,\ 0\leqslant U_{cpu} \leqslant 1
\\
\frac{W}{C}=ratio\ of\ wait\ time\ to\ compute\ time
$$
显然，基于资源最大化考虑，我们期望 $U_{cpu} \to 1$。

那么，对于计算密集型任务，随着计算占比的不断提高，其 $\frac{W}{C} \to 0$，因此 $N_{threads} \to N_{cpu}$ ；而对于 I/O 密集型任务，随着 I/O 等待占比的不断提高，其 $\frac{W}{C} \to \infin$ ，因此 $N_{threads} \to \infin$。



### 1.2 线程越多越好吗？

前面我们看到了，对于 I/O 占比较高的 I/O 密集型任务，理论公式中倾向于创建更多的线程来填补 CPU 的空闲，但这并不是零成本的。

关于线程所带来的开销，Eli Bendersky 在他的博文 [Measuring context switching and memory overheads for Linux threads](https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/) 中做了一些测量，

1. 上下文切换与启动的开销：

   ![](https://eli.thegreenplace.net/images/2018/plot-launch-switch.png)

​		可以看到在线程绑核切换下，上下文切换的开销每次大约 2us，线程启动开销大约 5us。

2. 内存开销：

   线程的内存开销主要体现在线程栈，他的[代码示例](https://github.com/eliben/code-for-blog/blob/master/2018/threadoverhead/threadspammer.c)表明，10000 个各持有 100KiB 实际栈空间消耗的线程，virtual memory 约为 80GiB（未实际分配），RSS 约为 80MiB。

Eli Bendersky 的文章主要想表达的是现代操作系统的线程开销已经非常小了，很多时候我们并不需要采用事件驱动等方式来增加复杂度，目前的操作系统支持数万个线程绰绰有余。

但假如我们想要数十万、上百万的线程呢？假如在不增加复杂度的前提下，能做到更低的开销呢？golang 用 goroutine 给出了解决方案。

在 Eli Bendersky 的文章中，测试线程切换的用例是让两个线程通过一个管道往复传递数据，结果是在一秒内，大概能来回传递 40 万次。而后他又顺手用 go 重写了[测试代码](https://github.com/eliben/code-for-blog/blob/master/2018/threadoverhead/channel-msgpersec.go)，得到的结果是：每秒 280 万次。

不论如何，无节制的创建新线程，最终一定会产生许多安全性问题，如过多的上下文切换，内存耗尽等等。



### 1.3 限制最大线程数

既然我们不能容忍无限制创建线程，那么最直接想到的自然是设定一个线程数上限，当线程超限后，拒绝再创建新的线程。

线程池是最通用的解决方案。

// 图 thread pool

池化是资源复用的常见方式，线程池可以最多持有 n 个工作线程（当然根据工作负载的变化，n 可以是动态的），同时持有一个任务队列。工作线程执行如下的循环：从队列获取任务 -> 执行任务 -> 再次从队列获取任务，因此如果没有空闲的工作线程，任务就必须在队列等待。

线程池不仅能限制线程的最大数量，同时也能降低线程反复创建、销毁产生的开销。对于突发的大规模任务也能比较优雅的实现降级、削峰填谷等措施。



### 1.4 换种思路

对于 I/O 密集型的任务，其中有很大一部工作，都是阻塞或等待，当 I/O 返回时会将任务唤醒。也正是这种阻塞唤醒的机制，给了我们创建多线程来提高 CPU 利用率的理由：阻塞中的线程不需要 CPU 时间。

设想假如我们将这种机制反过来，线程不是阻塞等待被唤醒，而是定期的去主动询问它所等待的 I/O，检查一下这个 I/O 是不是返回了。如果返回了，正好继续处理该任务，而如果没有返回，线程就可以去队列中拿一个新的任务来执行。

与阻塞唤醒的被动式相比，询问的方式会更加主动。线程不断的轮询队列、I/O、互斥量等等位置，尝试发现下发新任务、I/O 成功返回、互斥量被释放等等事件。一旦捕捉到了一个或多个事件，就选择某个感兴趣的事件，找到与之相关联的任务，切换到该任务继续执行，如此往复。

这种模式称之为事件驱动的并发编程模型，线程进行事件轮询的动作，称为事件循环。

// 图 event loop

从工作原理上我们就能发觉，事件驱动模型有如下的特点：

- 不需要很多线程：与多线程通过阻塞出让 CPU 相比，主动切换任务继续使用 CPU 资源，实际上是绕过了系统调度器
- 需要保存事件与任务的关联关系：通过事件可以找到对应的任务，才能实现任务切换
- 不允许存在阻塞：在多线程下对资源的阻塞等待，需要全部替换成非阻塞，否则一旦某个任务阻塞了线程，将导致该线程无法切换到其他任务。非阻塞的资源访问，如果操作系统不支持，就需要手工实现。

// callback 内容？

基于上述讨论，我们自然会发现：

1. 保存事件与任务的关联关系，等同于维护任务上下文
2. 请求资源时，非阻塞的将等待资源的任务换出，从被换出任务的角度看，就好像是自己被阻塞等待资源

上述两点已经覆盖了系统调度器的绝大部分工作内容（除了抢占，事件循环类似于协作式调度），而任务就类比了线程，不同点在于任务之间切换是协作式的（等待资源时主动出让线程），假如一个任务不主动出让线程，他就能永久的拥有该线程。对于这种任务，我们可以称之为协程（co-routine）。



## 2. 集中式调度器抽象存在什么问题？

## 3. 什么是 G-P-M 模型？

## 4. 如何实现调度？

## 5. 如何实现抢占？

