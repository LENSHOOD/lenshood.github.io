---
title: 对比 B+ Tree 文件组织 / LSM Tree 文件组织（第二篇：LSM Tree）
date: 2021-07-18 22:54:52
tags:
- b+ tree
- lsm tree
categories:
- DB
---

{% asset_img 1.jpg %}

B+ Tree 与 LSM Tree 是现今各类数据库中使用的比较多的两种数据结构，它们都可以作为数据库的文件组织形式，用于以相对高效的形式来执行数据库的读写。

本文简述了这两种数据结构的操作方式与操作开销，并对比了其自身的优缺点。

<!-- more -->

## LSM-Tree

LSM-Tree 最早是由 *Patrick O'Neil* 等人在 [*The Log-Structured Merge-Tree (LSM-Tree)*](https://www.cs.umb.edu/~poneil/lsmtree.pdf) 这篇论文中提出的，作者在论文中阐明：

*由于传统的 B-Tree 类型的索引，其实时维护（插入、删除）开销很高。因此提出了 LSM-Tree 这种基于磁盘的数据结构，来为在较长时间内产生高速文件写入（或删除）的场景提供低成本的索引。*

*LSM-Tree 采用对写入进行延迟、批次化的算法，通过类似合并排序的高效方法，将更改以级联的方式从内存逐步推进到一个或多个磁盘组件中。*

### 最初的 LSM-Tree

#### 问题之源

当我们大量的采用 B-Tree 及其变体这类数据结构来存储索引、数据等的时候，我们能通过这类平衡树获得不错的读效率。从查找角度讲只需要 `logN`的时间复杂度；从存储角度讲，结合 Buffer Pool， 我们能做到通常一次查询最多只需要一次 Random I/O。（以上内容详情可见本系列文的第一篇）

但为了维持这种高效读取所产生的代价就是：复杂的更新与随之带来的缓慢的更新耗时。

我们知道，对 B-Tree 类型的数据结构进行更新操作时，除了查找 node 所需的时间外，还可能涉及到 node 的 merge、spilt、上下层移动等操作，这些操作通常都是 Random I/O。同时，这类更新操作都是是即时发生（in-place）的，即当场发生，当场完成，旧数据会被直接替换掉。

{% asset_img 2.png %}

但我们早就已经知道一种最常用也是最简单的数据结构：日志（Log）。它结构非常简单，实现起来也容易，最重要的，由于对 log 文件的更新全部都是追加操作，是 Sequential I/O，对 HDD 磁盘结构很友好，写入速度会很快。

那么，我们能不能用 log 来替代 B-Tree 呢？如下的两个问题阻挡住了我们：

1. 查询效率差：由于插入的随机性，我们想要查找的数据可能会存在于 log 文件中的任何位置上
2. 空间利用率差：由于所有更新操作都是直接追加至 log 末尾，被更新的数据仍旧存在于更早的 log 中，我们需要采用非即时（out-of-place）的方式来将旧数据清理掉，但这种清理存在滞后性，这导致了空间利用率变差。

#### 归并更新的日志树结构：LSM-Tree

前述论文中首先假设了如下的一种数据结构（最基础的 LSM-Tree）：

{% asset_img 3.png %}

所有数据分成两个 Components 存放在 memory 和 disk 中，其中 memory 中的 Component 记为 $C_0$，disk 中的 Component 记为 $C_1$。$C_0$ 相对$C_1$而言更小一些。

考虑到性能与可用性，一些常见的实践并没有在图上给出，如：

- 仍然会通过 WAL 来进行恢复
- $C_1$ 仍然会采用 Buffer Pool 来提升读写性能

上述 LSM-Tree 在有数据写入时，新增数据首先写入到 $C_0$，之后会在一定时间的 delay 后，合并（merge）入 $C_1$。而在对数据查询时，会先在 $C_0$中查找，找不到再去 $C_1$。

在具体的 Component 内部，其数据结构采用了树形结构来存放：

- $C_0$ 作为存放在 memory 中的结构，不产生 I/O 消耗，不需要按 Page 或是 Batch 存取，因此采用了2-3-Tree 或 AVL-Tree 这类的平衡树。
- $C_1$ 作为存放在 disk 中的结构，仍旧采用了传统的 B-Tree 结构的变体（类似 SB-Tree），包括对顺序查询优化，单个节点可全满（100% full），页打包为多页块（multi-page block）来提升磁盘臂效率。

### 二阶（two components）LSM-Tree 的操作

#### 插入

在整个数据结构最初的时候，并没有数据，因此刚开始的插入都只会影响 $C_0$，而不会影响$C_1$。如下图所示：

{% asset_img 4.png %}

随着数据的不断增加，$C_0$ 的容量达到了阈值：

{% asset_img 5.png %}

之后会开始第一次合并，从左侧树开始，合并一部分数据至 $C_1$。整个合并过程采用的是逐步合并的方式，一次合并只搬移一部分数据。

{% asset_img 6.png %}

> 上图中对整个流程进行了一些简化，实际上从 $C_0$ 移动的数据会先进入 Buffer Pool，最后由 Buffer Pool 选择何时写入 Disk

在经过了一段时间后，$C_0$ 容量又一次触发阈值，需要将数据再次合并至 $C_1$：

{% asset_img 7.png %}

这里的关键之处在于，$C_0$ 中被选择合并的部分已经移出，但在 $C_1$ 中，合并后的新节点，直接追加在其尾部，而最左侧被合并的部分并没有被删掉，只是做了标记（虚线）。

正因为新节点直接追加，因此写入速度很快，而父节点中虽然需要更新指针，但因为 Buffer Pool 的存在，除了叶节点以外，其内部节点都可以保存在 Buffer Pool 中，更新它们也就没有 I/O 消耗。

在整个合并流程彻底完成后，$C_1$最左侧的冗余数据将会被异步的删掉。

之后随着数据不断的插入，合并不断的进行，$C_1$ 中被合并的部分也不断的被选取为更右侧的树枝，这一过程称为滚动合并（rolling-merge）。

#### 查找

查找操作从原理上讲就是先查找 $C_0$， 找不到就再查找 $C_1$。

通过观察我们能得知，最近插入的数据，其被访问的概率、频次会更高（LRU ）。正因为 $C_0$ 存放的都是相对更新、距离插入时间更近的数据，因此$C_0$ 能够有效的提升查询效率，从这个角度看，$C_0$ 在查询中更像是一个缓冲区。

#### 删除、更新

由于 LSM-Tree 这种结构，删除动作可以像插入一样高效：

{% asset_img 8.png %}

先在 $C_0$ 中查找被删除 entry 应该所在的位置，若$C_0$ 中不存在这一 entry，那么插入一个删除标记（tombstone），若存在则替换。在之后的查找中，只要发现了该标记，就可认为对应的 entry 不存在。随着合并的进行，删除标记被合并至$C_1$，此时如果 $C_1$ 中的确存在该 entry，那么将其删除即可。

而对于 LSM-Tree 结构下的更新操作，实际上与插入操作没有本质的区别。

### n 阶 LSM-Tree

由于 LSM-Tree 这种结构同时使用到了 Memory 和 Disk，即 mem 资源与 I/O 资源。那么怎么样对 LSM-Tree 进行设计和调优，才能达到理论最佳呢？

论文中定义了一种指标：**批次合并参数 M（The Batch-Merge Parameter M）**。

全局上看，插入成本（insert cost）主要体现在滚动合并的过程中。因此定义 $M$ 为滚动合并中，插入到 $C_1$ 树的每个单页叶节点中的 $C_0$ 树的平均 entreis 数量。即：

$M = (S_p / S_e) \cdot (S_0/(S_0+S_1))$

其中，

$S_e = $ 单个 entry 的 size（以 byte 计）

$S_p=$ Page size（以 byte 计）

$S_0 = $ $C_0$ 的 leaf level 的 size，（以 MByte 计）

$S_1 =  $ $C_1$ 的 leaf level 的 size，（以 MByte 计）

所以 $(S_p / S_e)$ 就是单页可存放的 entry 数量，$(S_0/(S_0+S_1)$ 是 $C_0$ 中数据占总数据量的占比。

举例说明：

通常的实现中，$S_1 = 40 \cdot S_0$，$S_p / S_e = 200$， 因此 $M = 5$。

基于上述内容，我们可以知道，$M$ 越大，平均合并的 $C_0$ entry 越多，效率就越高。而假如 $C_1$ 远大于 $C_0$ 或者单个 entry 巨大导致单个页只能存放少量的几个 entries，那么就会导致 $M$ 很小，甚至可能产生 $M < 1$ 的情况。

#### 怎么在成本最低的情况下让 $M$ 达到最大

前面讲到， $C_1$ 与 $C_1$ 的大小差距越悬殊，$M$ 会越小，所以我们就期望能在资源允许的情况下，尽可能的增大 $C_0$，来增大 $M$。

从成本角度看，整个 LSM-Tree 的成本包括：

- 内存空间成本
- 磁盘 I/O 成本

为了找到最小成本点，我们首先选取一个很大的 $C_0$，这种情况下，I/O 速率会相对较低。之后我们逐步的缩小 $C_0$，用昂贵的内存空间换取便宜的磁盘空间。一直到 I/O 速率达到全速状态。在这之后如果继续减少 $C_0$ 的容量，就会导致 I/O 延迟加大。

实际上，即使是按照上述方式所选取出的 $C_0$ 容量，如果在数据量稍大的场景下，也是十分庞大的，这就会导致内存投入过于昂贵。

#### 扩展至 n 阶

从性能、成本的定性分析上我们知道，考虑到成本的限制，二阶的 LSM-Tree 其 $C_0$ 和 $C_1$ 之间的容量差距还是太大了，那么一种缓解的办法就是在  $C_0$ 和 $C_1$ 之间插入更多的中间层。

{% asset_img 1.jpg %}

这样每一层的部分都与二阶一样，不断的向下一层合并。最终合并到最后一层。

此外，在论文给出的定理 3.1中证明了：每一层之间的容量比例为固定值时，整体滚动合并所产生的 I/O 速率（最小化速率等同于最小化 I/O 成本）最小。（相关证明可见论文 3.4 节）



## LevelDB 中的 LSM-Tree

