---
title: 对比 B+ Tree 文件组织 / LSM Tree 文件组织（第二篇：LSM Tree）
date: 2021-07-18 22:54:52
tags:
- b+ tree
- lsm tree
categories:
- DB
---

{% asset_img 1.jpg %}

B+ Tree 与 LSM Tree 是现今各类数据库中使用的比较多的两种数据结构，它们都可以作为数据库的文件组织形式，用于以相对高效的形式来执行数据库的读写。

本文简述了这两种数据结构的操作方式与操作开销，并对比了其自身的优缺点。

<!-- more -->

## LSM-Tree

LSM-Tree 最早是由 *Patrick O'Neil* 等人在 [*The Log-Structured Merge-Tree (LSM-Tree)*](https://www.cs.umb.edu/~poneil/lsmtree.pdf) 这篇论文中提出的，作者在论文中阐明：

*由于传统的 B-Tree 类型的索引，其实时维护（插入、删除）开销很高。因此提出了 LSM-Tree 这种基于磁盘的数据结构，来为在较长时间内产生高速文件写入（或删除）的场景提供低成本的索引。*

*LSM-Tree 采用对写入进行延迟、批次化的算法，通过类似合并排序的高效方法，将更改以级联的方式从内存逐步推进到一个或多个磁盘组件中。*

### 最初的 LSM-Tree

#### 问题之源

当我们大量的采用 B-Tree 及其变体这类数据结构来存储索引、数据等的时候，我们能通过这类平衡树获得不错的读效率。从查找角度讲只需要 `logN`的时间复杂度；从存储角度讲，结合 Buffer Pool， 我们能做到通常一次查询最多只需要一次 Random I/O。（以上内容详情可见本系列文的第一篇）

但为了维持这种高效读取所产生的代价就是：复杂的更新与随之带来的缓慢的更新耗时。

我们知道，对 B-Tree 类型的数据结构进行更新操作时，除了查找 node 所需的时间外，还可能涉及到 node 的 merge、spilt、上下层移动等操作，这些操作通常都是 Random I/O。同时，这类更新操作都是是即时发生（in-place）的，即当场发生，当场完成，旧数据会被直接替换掉。

{% asset_img 2.png %}

但我们早就已经知道一种最常用也是最简单的数据结构：日志（Log）。它结构非常简单，实现起来也容易，最重要的，由于对 log 文件的更新全部都是追加操作，是 Sequential I/O，对 HDD 磁盘结构很友好，写入速度会很快。

那么，我们能不能用 log 来替代 B-Tree 呢？如下的两个问题阻挡住了我们：

1. 查询效率差：由于插入的随机性，我们想要查找的数据可能会存在于 log 文件中的任何位置上
2. 空间利用率差：由于所有更新操作都是直接追加至 log 末尾，被更新的数据仍旧存在于更早的 log 中，我们需要采用非即时（out-of-place）的方式来将旧数据清理掉，但这种清理存在滞后性，这导致了空间利用率变差。

#### 归并更新的日志树结构：LSM-Tree

前述论文中首先假设了如下的一种数据结构（最基础的 LSM-Tree）：

{% asset_img 3.png %}

所有数据分成两个 Components 存放在 memory 和 disk 中，其中 memory 中的 Component 记为 `C0`，disk 中的 Component 记为 `C1`。`C0` 相对`C1`而言更小一些。

考虑到性能与可用性，一些常见的实践并没有在图上给出，如：

- 仍然会通过 WAL 来进行恢复
- `C1` 仍然会采用 Buffer Pool 来提升读写性能

上述 LSM-Tree 在有数据写入时，新增数据首先写入到 `C0`，之后会在一定时间的 delay 后，合并（merge）入 `C1`。而在对数据查询时，会先在 `C0`中查找，找不到再去 `C1`。

在具体的 Component 内部，其数据结构采用了树形结构来存放：

- `C0` 作为存放在 memory 中的结构，不产生 I/O 消耗，不需要按 Page 或是 Batch 存取，因此采用了2-3-Tree 或 AVL-Tree 这类的平衡树。
- `C1` 作为存放在 disk 中的结构，仍旧采用了传统的 B-Tree 结构的变体（类似 SB-Tree），包括对顺序查询优化，单个节点可全满（100% full），页打包为多页块（multi-page block）来提升磁盘臂效率。

### 二阶（two components）LSM-Tree 的操作

#### 插入

在整个数据结构最初的时候，并没有数据，因此刚开始的插入都只会影响 `C0`，而不会影响`C1`。如下图所示：

{% asset_img 4.png %}

随着数据的不断增加，`C0` 的容量达到了阈值：

{% asset_img 5.png %}

之后会开始第一次合并，从左侧树开始，合并一部分数据至 `C1`。整个合并过程采用的是逐步合并的方式，一次合并只搬移一部分数据。

{% asset_img 6.png %}

> 上图中对整个流程进行了一些简化，实际上从 `C0` 移动的数据会先进入 Buffer Pool，最后由 Buffer Pool 选择何时写入 Disk

在经过了一段时间后，`C0` 容量又一次触发阈值，需要将数据再次合并至 `C1`：

{% asset_img 7.png %}

这里的关键之处在于，`C0` 中被选择合并的部分已经移出，但在 `C1` 中，合并后的新节点，直接追加在其尾部，而最左侧被合并的部分并没有被删掉，只是做了标记（虚线）。

正因为新节点直接追加，因此写入速度很快，而父节点中虽然需要更新指针，但因为 Buffer Pool 的存在，除了叶节点以外，其内部节点都可以保存在 Buffer Pool 中，更新它们也就没有 I/O 消耗。

在整个合并流程彻底完成后，`C1`最左侧的冗余数据将会被异步的删掉。

之后随着数据不断的插入，合并不断的进行，`C1` 中被合并的部分也不断的被选取为更右侧的树枝，这一过程称为滚动合并（rolling-merge）。

#### 查找

查找操作从原理上讲就是先查找 `C0`， 找不到就再查找 `C1`。

通过观察我们能得知，最近插入的数据，其被访问的概率、频次会更高（LRU ）。正因为 `C0` 存放的都是相对更新、距离插入时间更近的数据，因此`C0` 能够有效的提升查询效率，从这个角度看，`C0` 在查询中更像是一个缓冲区。

#### 删除、更新

由于 LSM-Tree 这种结构，删除动作可以像插入一样高效：

{% asset_img 8.png %}

先在 `C0` 中查找被删除 entry 应该所在的位置，若`C0` 中不存在这一 entry，那么插入一个删除标记（tombstone），若存在则替换。在之后的查找中，只要发现了该标记，就可认为对应的 entry 不存在。随着合并的进行，删除标记被合并至`C1`，此时如果 `C1` 中的确存在该 entry，那么将其删除即可。

而对于 LSM-Tree 结构下的更新操作，实际上与插入操作没有本质的区别。

### K 阶 LSM-Tree



## LevelDB 中的 LSM-Tree

