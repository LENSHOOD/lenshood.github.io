---
title: Spanner: Google 的全球分布式数据库
date: 2020-11-06 17:40:44
tags:
- database
- spanner
categories:
- TiDB
---

> 本文是对 《Spanner: Google’s Globally-Distributed Database》的翻译，原文请见：https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf

# 摘要

​        Spanner 是 Google 的可伸缩、多版本、全球分布且同步复制的数据库。它是首个在全球范围分布数据且支持外部一致性分布式事务的系统。本文描述了 Spanner 的结构、特性集、其多项设计决策下蕴含的理论基础以及一个能够暴露时钟不确定性（clock uncertainly）的新颖的时间 API。该 API 及其实现是作为关键角色来支撑外部一致性和其他许多涉及整个 Spanner 的强大特性，如对过往数据的非阻塞读（nonblocking read in the past），无锁的读事务，原子的 schema 变更等。

# 1 介绍

​        Spanner 是由 Google 设计、构造并部署的可伸缩全球分布式数据库。其最顶层的抽象，可以看做是一个在遍布世界各地的许多 Paxos [21] 集合之间分片数据的数据库。复制用于全局可用性以及地理位置性；客户端自动在不同的副本间故障转移。当数据量或服务器数量发生变化时，Spanner 会自动在机器之间重分片（reshard）数据，并且能在机器之间自动迁移数据（哪怕是跨数据中心）来平衡负载和应对故障。Spanner 被设计为能够在数百个数据中心的数万亿个数据库行之间扩展到数百万台机器。

​        应用程序能够使用 Spanner 来实现高可用，即使面对大面积的自然灾害，它也能靠在洲际内甚至跨洲际来进行数据复制。我们的内部客户是 F1 [35]，一个重写的 Google 广告后端。F1 使用了遍布美国的五个副本。大多数其他的应用大概会将他们的数据复制到 3 到 5 个处于同一个地理区域内数据中心，但故障模式相对独立。即只要他们能够在 1 到 2 个数据中心失效的情况下存活下来，大多数应用都会在低延迟与高可用之间选择低延迟。

​        Spanner 主要聚焦于跨数据中心复制数据的管理，但我们也花了非常多精力在我们的分布式基础设施之上设计和实现重要的数据库特性。即使大多数项目都乐于使用 BigTable [9]，但我们也持续的收到了用户的抱怨：BigTable 难以使用在某些较为复杂、schema 经常演进，或是想要在大面积复制中保持强一致性的应用中。（其他作者也有类似的说法 [37]。）尽管存在相对较弱的写吞吐，Google 的许多应用程序仍然会选择使用 Megastore [5]，因为其半关系型数据模型以及支持同步复制的特性。因此，Spanner 从一个类 BigTable 的有版本 kv 存储（versioned key-value store）进化为一个基于时间版本的多版本数据库。数据存储在 schema 化的半关系型表中；数据有版本，且每一个版本都自动以其提交时间作为时间戳；旧版本的数据受可配置的垃圾收集策略所控制；应用也可以读取旧时间戳上的数据。Spanner 支持通用事务，且提供了一个基于 SQL 的查询语言。

​        作为一个全球分布的数据库。Spanner 提供了许多有趣的特性。首先，对于数据复制的配置可以被应用程序细粒度的动态控制。应用能够指定一些约束（constrains）来控制哪个数据中心包含哪些数据，数据距离用户有多远（来控制读延迟），副本之间有多远（来控制写延迟），以及维护了多少份副本（来控制持久性、可用性和读性能）。为了均衡数据中心之间的利用率，数据能够动态且透明的在不同数据中心之间移动。其次，Spanner 提供了两个在分布式数据库中较难实现的特性：提供外部一致性 [16] 读写，和基于时间戳的跨数据中心全球一致性读。这些特性使得 Spanner 能支持一致性备份，执行一致性 MapReduce [12]，以及原子的 schema 更新，所有这些都是全球尺度，甚至是在正在执行事务时。

​        这些特性都基于一个事实即 Spanner 会给事务分配在全球都有意义的提交时间戳，即使该事务可能是分布式的。时间戳反映了串行顺序。进一步的，串行顺序满足了外部一致性（或等价性，线性化 [20]）：如果事务 T1 在另一个事务 T2 开始之前提交，那么 T1 的提交时间戳便小于 T2。Spanner 是首个在全球尺度提供如此承诺的系统。

​        使能这种属性的关键是一个新的 TrueTime API 和其实现。该 API 直接将时钟不确定性暴露出来，而对 Spanner时间戳的承诺依赖于实现提供的边界。假如这种不确定性很大，Spanner 就会停下来等待直到不确定性消失。Google 的集群管理软件提供了 TrueTime API 的实现。这个实现采用多个现代时钟基准（GPS 和原子钟）来确保不确定性很小（通常小于 10ms）。

​        第二节描述了 Spanner 实现的结构、特性集，和设计包含的工程决策。第三节描述了我们的新型 TrueTime API 和其实现的概览。第四节描述 Spanner 是如何使用 TrueTime 来实现外部一致性分布式事务、无锁只读事务以及原子 schema 更新的。第五节提供了一些对 Spanner 性能和 TrueTime 行为的测试，并讨论了 F1 的经验。第六、七、八节描述了相关未来工作，以及对我们结论的总结。

# 2 实现

​		这一节描述了 Spanner 的结构以及蕴含在实现下的理论基础。之后描述了目录抽象 - 它用于管理复制和局部性，是数据移动的基本单元。最后，描述了我们的数据模型、为什么 Spanner 看起来更像是关系型数据库而不是 k-v 存储，以及应用程序如何控制数据的局部性。

​		Spanner 的一份部署被称为一个 universe。鉴于 Spanner 全球管理数据的特点，将只有少数个 universe 在运行。目前我们运行了一个 测试/演示 universe，一个开发/生产 universe 和一个仅用于生产的 universe。

​		Spanner 被组织为一组 zone 集合，每一个 zone 都是一份大致类似 BigTable 的服务器部署 [9]。Zone 是管理部署的单元。这一组 zone 集合也是一组数据可被复制的位置集合。当新的数据中心被引入服务而旧的被关闭时，zone 能够在运行的系统中被添加或移除。Zone 也是物理隔离的单元：如果不同的应用程序的数据必须在同一数据中心的不同的服务器集上进行分区，那么在一个数据中心内就有可能分布一个或多个 zone。

{% asset_img figure-1.png %}

​		Figure 1 展示了在 Spanner universe 中的服务器。一个 zone 包含一个 zonemaster 和从一百至数千个 spanserver。zonemaster 给 spanserver 分配数据；spanserver 将数据提供给客户端。每个 zone 包含的 location proxy 用于客户端来定位能够提供数据的 spanserver。universe master 和 placement driver 目前都是单实例。universe master 主要作为一个控制台来展示所有 zone 的状态信息以用于交互式调试。placement driver 以分钟为单位处理跨 zone 的数据自动移动。placement driver 周期性的与 spanserver 通信，来寻找需要被移动的数据，以满足复制约束（replication constrain）被更新的情况或是负载均衡。篇幅考虑，我们将只详细描述 spanserver。

## 2.1 Spanserver 软件栈

{% asset_img figure-2.png %}

​		这一节聚焦于 spanserver 的实现，以说明复制和分布式事务在基于 BigTable 的实现上是如何分层的。Figure 2 展示了软件栈。在最底层，每一个 spanserver 都负责 100 到 1000 个称为 tablet 的数据结构的实例。一个 tablet 类似于 BigTable 的 tablet 抽象，因为它实现了如下映射包（mapping bag）：

​		`(key:string, timestamp:int64) → string`

​		与 BigTable 不同，Spanner 会给数据分配时间戳，这使得 Spanner 更像是一个多版本数据库而不只是一个 k-v 存储。一个 tablet 的状态会被保存在类 B-Tree 结构的文件以及写前日志文件中（write-ahead log），所有这些文件都被存放在分布式文件系统 Colossus（Google File System 的继任者 [15]） 内。

​		为了支持复制，每个 spanserver 都在每一个 tablet 上实现了一个 Paxos 状态机。（在 Spanner 的早期版本中，每个 tablet 能支持多个 Paxos 状态机，这能让复制配置更加灵活。但这种设计的复杂性让我们放弃了它。）每个状态机都将它的元数据以及日志存储在相关的 tablet 上。我们的 Paxos 实现支持基于时间租约的久存活（long-lived）leader，该租约默认为 10 秒。当前的 Spanner 实现会在日志中记录 Paxos 写两次：一次是在 tablet 的日志中，一次是在 Paxos 的日志中。这种选择是一个权宜之计，我们最终很可能会补救它。我们的 Paxos 实现是流水线的，以便在存在 WAN 延迟的情况下提升 Spanner 的吞吐；但是由 Paxos 应用的写入是按顺序的（我们将在第四节中依赖的一个事实）。

​		Paxos 状态机用于实现对映射包的一致性复制。每个副本的 k-v 映射状态都存储在它对应的 tablet 中。对状态的写操作必须由 Paxos leader 发起；访问状态时则直接从任意足够新的副本中的 tablet 内读取。副本集共同组成一个 Paxos 组。

​		在每个作为 leader 的副本中，spanserver 都实现了一个 lock table 用于并发控制。该 lock table 包含了两阶段锁（two-phase locking）的状态：他将一定范围的 key 映射到锁状态上。（注意一个久存活的 Paxos leader 对提升 lock table 的效率至关重要。）在 BigTable 和 Spanner 中，我们都是为久存活事务而设计的（例如生成报告，可能会需要以分钟为单位的顺序），这会导致当存在冲突时，乐观并发控制的性能较差。对需要同步的操作，例如事务读，会在 lock table 请求锁，其他按操作则会绕过 lock table。

​		在每个作为 leader 的副本中，spanserver 也都实现了一个事务管理器来支撑分布式事务。事务管理器用于实现一个 leader 参与者；而组内的其他副本则被称为 slave 参与者。如果一个事务只引入了一个 Paxos 组（大多数事务都是这样），由于 lock table 和 Paxos 组共同提供了事务性，因此会自动绕过事务管理器。假如一个事务引入了多于一个 Paxos 组，这些组的 leader 就会协调进行两阶段提交。其中的某个参与组被选为协调者：该组的 leader 参与者会被成为 leader 协调者，而组内的其他 slave 就成了 slave 协调者。每个事务管理器的状态都保存在其下的 Paxos 组中（所以也会一并被复制）。

## 2.2 目录和放置（placement）

​		在 k-v 映射包之上，Spanner 实现支持一个称为目录的桶抽象，它是由一组相邻的 key 共享一个通用的前缀。（选择术语目录是一个历史性意外，一个更好的术语应该是桶（bucket）。）我们会在 2.3 节解释前缀的来源。支持目录允许应用程序控制通过仔细的选择 key 来控制数据的位置。

{% asset_img figure-3.png %}

​		目录是数据放置的单元。一个目录中的所有数据都拥有相同的复制配置。Figure 3 展示了当数据在 Paxos 组之间移动时，是以目录为单位来移动的。Spanner 可能会通过移动一个目录来降低某个 Paxos 组的负载；将经常访问的目录放在同一个组中；或移动一个目录到距离访问者更近的地方。当客户端操作在进行中时也能够移动目录。可以预期一个 50MB 的目录能够在几秒钟内被移动。

​		一个 Paxos 组可能会包含多个目录的事实隐含了 Spanner 的 tablet 与 BigTable 的 tablet 的不同：Spanner 的 tablet 并不一定是单个按字符顺序分区的行空间。相反，Spanner 的 tablet 是一个可能包含多个行空间分区的容器。我们做出这样的决定是为了能够将经常访问的多个目录放在一起。

​		`Movedir` 是一个用于在 Paxos 组间移动数据的后台任务 [14]。由于 Spanner 还不支持基于 Paxos 的配置更改，`Movedir`也用于给 Paxos 组添加或移除副本 [25]。`Movedir` 并不被实现为单个事务，因此能它能避免在大量数据移动时阻塞正在进行中的读写。当它移动了所有的数据后，它会使用事务来原子对 “名义上的数据量” 进行移动，并更新被移动双方的 Paxos 元数据。

​		目录也是应用程序能够指定其地理复制属性（简称放置）的最小单元。我们的放置规范语言（placement-specification language）的设计分离了管理复制配置的责任。管理员可以控制两个维度：副本的数量和类型，以及对这些副本放置的地理位置。他们在这两个维度创建了命名选项的菜单（例如，North America, replicated 5 ways with 1 witness）。一个应用通过给每个数据库和/或独立的目录打标签来控制数据如何被复制，标签的内容就是上述选项的组合。例如，一个应用也许想将每个端用户的数据存储在他自己的目录下，那么用户 A 的数据可以有三个副本在欧洲，而用户 B 的数据可以有五份副本在北美。

​		为了解释清晰，我们简化了整个流程。实际上，当一个目录变得过大时，Spanner 会将其分片至多个片段（fragment）中。片段可能会从不同的 Paxos 组而来（也即不同的服务器中）。`Movedir` 实际上在组间是移动片段而不是整个目录。

## 2.3 数据模型

​		Spanner 将如下数据集特性暴露给应用程序：一个基于模式化半关系型表的数据结构，一种查询语言，以及通用事务。为了支持这些特性所做的工作受到了许多因素的推动。对于支持模式化半关系型表和同步复制的需求，Spanner 得到了流行的 Megstore 的推动 [5]。Google 内部至少有 300 个应用程序在使用 Megastore（即使它的性能相对较低），原因是它的数据模型比 BigTable 更容易管理，且支持跨数据中心的同步复制。（BigTable 只支持跨数据中心最终一致性。）著名的 Google 应用使用 Megasotore 的例子有 Gmail、Picasa、Calendar、Android Market 和 AppEngine。鉴于Dremel [28] 作为一种交互式数据分析工具的流行，在 Spanner 中支持类 SQL 查询语言的需求也很明确。最后，由于 BigTable 中跨行事务的缺失导致了频繁的抱怨；Percolator [32] 的构建部分是为了解决这个问题。一些作者声称，由于会引入性能或可用性的问题  [9, 10, 19]，对通用的两阶段提交的支持过于昂贵。我们认为，由于滥用事务而导致瓶颈出现时，最好让应用程序员来处理性能问题，而不是总围绕着缺失事务来编码。在 Paxos 上运行两阶段提交可以缓解可用性问题。

​		应用数据模型层建立在实现所支持的目录-桶 k-v 映射上。一个应用能在 universe 中创建一个或多个数据库。每一个数据库都能包含无限数量的模式化表。表看起来就像是关系型数据库的表一样，有行、列和有版本的值。我们不会深入讲解 Spanner 的查询语言。它与 SQL 很像，且包含了一些扩展来支持协议缓冲区值字段。

​		Spanner 的数据模型不是纯关系型的，因为行必须要有名称。更准确的讲，每个表都要求要有一个由一个或多个主键列组成的有序集合。这种要求使得 Spanner 仍然起来像 k-v 存储：主键构成了行的名称，每个表都定义了一个从主键列到非主键列的映射。只有当行的键（row's key）定义为某些值（即使它是 NULL）时，该行才存在。施加这种结构非常有用，因为这使应用程序通过选择 key 值来控制数据的位置。

{% asset_img figure-4.png %}

Figure 4 包含了一个 Spanner 为每一个用户及每一个相册存储图片元数据模式的例子。模式语言与 Megastore 类似，除此之外，每个 Spanner 数据库都要求必须由客户端划分为一个或多个表层次结构。客户端应用程序通过 `INTERLEAVE IN` 在数据库声明层级结构。层级之上的表是一个目录表。目录表中键为 `K` 的每一行，以及派生表中按字典顺序以`K`开头的所有行，构成一个目录。`ON DELETE CASCADE`是说当目录表中的一行被删除时，任何关联的子行也会被删除。该图也展示了示例数据库的交错布局：`Albums(2,1) ` 表示`user id 2`,`album id 1` 的 `Albums` 表中的行。这种将表交错以形成目录的方法非常重要，因为这允许客户端来描述多个表之间的位置关系，这对于在分片的、分布式数据库中获得良好性能是必要的的。如果没有这种方法，Spanner 就无法知道最重要的位置关系了。

# 3 TrueTime 

{% asset_img table-1.png %}

​		这一节描述了 TrueTime API 和其实现的概要。我们将更多的细节留给另一篇文章：我们的目的是展示拥有这种 API 的能力。Table 1 列出了该 API 的方法。TrueTime 显式将时间表示为一个 *TTinterval* ，即一个包含有界时间不确定性的时间间隔（与标准时间接口不同，标准时间接口没有给客户带来不确定性这一概念）。*TTinterval* 的端点来源于 *TTstamp* 类型。*TT.now()* 方法返回一个 *TTinterval*，改结果能保证包含在 *TT.now()* 被调用时的绝对时间。该时间纪元类似于带有闰秒补偿（leap-second smearing）的 UNIX 时间。将瞬时误差边界定义为 *ε*，即区间宽度的一半，平均误差边界为 *(bar ε)*。*TT.after()* 和 *TT.before()* 是对 *TT.now()* 包装而成的简便方法。

​		用函数 *t abs(e)* 表示事件 *e* 的绝对时间。更正式的表述是，TrueTime 确保对于一次  *tt = TT.now()* 的调用，*tt.earliest ≤ t abs(e now) ≤ tt.latest* ，*e now* 是调用事件。

​		TrueTime 底层的时间参考是 GPS 和原子钟。TrueTime 使用两种时间参考的原因是它们有不同的故障模式。GPS 参考源漏洞包括天线和接收器错误，本地无线电干扰，相关性故障（比如，类似错误的闰秒处理及欺骗的设计错误），以及 GPS 系统中断故障。原子钟可能会以与 GPS 不相关的方式失效，在很长的一段时间内，由于频率误差，原子钟会产生显著的漂移。

​		TrueTime 由每个数据中心一组的 time master 和每台机器一个的 time slave 守护程序组成。大多数 master 都带有专用天线的 GPS 接收器；这些 master 被物理分离，以减小天线失效、无线电干扰与欺骗的影响。其余的 master（我们称之为世界末日（Armageddon）master）配备了原子钟。一台原子钟并没有那么贵：一个世界末日 master 的花费与一个 GPS master 处于同一数量级。所有 master 的时间参考会定期互相对比。每个 master 也会交叉检查它的时间参考与它本地时间前进的速率，如果存在较大的分歧，则会将自己移除。在同步间，世界末日 master 会显示出源于保守的最坏情况的时钟漂移而缓慢增加的时间不确定。GPS master 则显示不确定性通常接近于零。

​		每个守护程序都会从多个 master 轮询数据 [29] 来降低任意 master 错误的可能性。一部分是从附近数据中心选取的 GPS master；其余的是较远的数据中心的 GPS master，还有一些是世界末日 master。守护程序应用 Marzullo 算法的一个变体来探测并拒绝假信息，并将本地机器时钟与正确的 master 同步。为了防止损坏本地时钟，那些显示频率偏移大于组件规格和操作环境的最坏情况边界的机器将被移除。

​		在同步间隔中，一个守护程序显示为缓慢的增加时间不确定性。*ε* 源于保守应用的最坏情况下的本地时钟漂移。*ε* 也依赖于 time master 的不确定性和与 time master 通信的延迟。在我们的生产环境，*ε* 通常是时间的锯齿函数，在每个轮询间隔内，从 1 到 7ms 不等。 因此 *(bar ε)* 在大多数情况下是 4ms。守护程序的轮询间隔目前是 30 秒，且目前应用的漂移率被设定为 200us/s， 这两个值共同构成了 0 到 6ms 的锯齿边界。剩下的 1ms 是从与 time master 的通信延迟而来。当出现故障时，这种锯齿可能会有偏差。例如，time master 的偶尔不可用会导致数据中心范围的 *ε* 的增加。同样的，过载的机器和网络连接也会导致 *ε* 的局部峰值。

# 4 并发控制
​		这一节描述 TrueTime 是如何用于保证并发控制属性的正确性，以及这些属性是如何用于实现类似外部一致性事务、无锁只读事务以及旧数据非阻塞读等特性的。这些特性能够使能例如保证在时间戳*t*下的全数据库审计读能看到截止*t*时提交的所有事务。

​		进一步的，区分 Paxos 能看到的写（除非上下文清晰，否则我们称为 Paxos 写）与 Spanner 客户端产生的写是很重要的。例如，两阶段提交会在准备阶段生成一个 Paxos 写，而这个写操作与 Spanner 客户端写并没有关系。

## 4.1 时间戳管理
​		表 2 列出了Spanner 支持的操作类型。Spanner 支持了读写事务、只读事务（预声明快照隔离事务），以及快照读。单机写被实现为读写事务；非快照单机读被实现为只读事务。二者都会内部重试（客户端不需要再编写他们自己的重试循环）。
只读事务是一种具有快照隔离 [6] 性能优势的事务。只读事务必须提前声明不包含任何写；并不能简单认为它是一种不包含写操作的读写事务。只读事务中的读会在系统选择的时间戳下无锁的执行，因此不会阻塞到来的写操作。在只读事务中执行的读操作可以在任意满足最新数据的副本上执行（4.1.3 节）。
快照读是在过去的数据上执行无锁的读。客户端可以为快照读指定一个时间戳，也可以提供一个期望的过期时间戳的上界来让 Spanner 选择时间戳。这两种情况下快照读都可以在任意满足最新数据的副本上执行。
无论是只读事务还是快照读，一旦选定了时间戳，提交就不可避免，除非是在该时间戳上的数据已经被垃圾回收了。因此，客户端可以避免在一个重试循环中暂存结果。当一个服务器失效时，客户端会在内部通过重复时间戳和当前读位置来继续将该操作执行在一个不同的服务器上。

### 4.1.1 Paxos Leader 租约
​		Spanner 的 Paxos 实现使用定时租约来实现 leader 久存活（默认10秒）。一个潜在的leader发送定时租约投票请求；在收到额定数量的租约投票时，leader就能知道它拥有了租约。在一次成功的写之后，副本会隐含的自动延长该租约投票，且当租约投票将要到期时，它会请求延长租约投票。当某个副本探测到它拥有了额定的租约投票数量时，就可定义一个 leader 租约期的开始，而当它不再拥有额定的租约投票时（因为有一些投票过期了）可定义 leader 租约期结束。
Spanner 依赖如下离散不变性：对每个 Paxos 组，每个 Paxos leader 的租约期与其他 leader 的租约期不相交。附录 A 描述了这种不变性是如何强制保证的。
Spanner 的实现允许一个 Paxos leader 通过释放 slave 的的租约投票来放弃 leader 角色。为了保持离散不变性，Spanner 会在放弃 leader 被允许时做出约束。定义 *Smax* 为 leader 使用的最大时间戳。后续章节会描述何时 *Smax* 会增加。在放弃 leader 之前，leader 必须等待 *TT.after(Smax)* 为 True。

### 4.1.2 为读写事务分配时间戳
​		事务读写使用两阶段锁。因此，可以在所有锁都被获取之后，以及任意锁被释放之前的任何时间点分配时间戳。对于给定的事务，Spanner 会给它分配 Paxos 分配给代表事务提交的 Paxos write 操作的时间戳。

​		Spanner 依赖下述单调不变性：在每个 Paxos 组中，Spanner 都使用单调递增的顺序给 Paxos write 分配时间戳，哪怕是跨 leader 的情况。一个单 leader 的副本可以简单的按单调递增分配时间戳。这种不变性通过使用如下离散不变性来强制跨 leader：一个 leader 必须仅在它的租约期内分配时间戳。注意无论何时一个时间戳 *s* 被分配，*Smax* 都会被提升至 *s* 以此来保持离散性。

​		Spanner 也强制如下外部一致不变性：如果一个事务 *T2* 在事务 *T1* 提交后开始，那么 *T2* 的提交时间戳必须要大雨 *T1* 的提交时间戳。定义事务 *Ti* 的开始和提交事件为 *e start i* 和 *e commit i*，*Ti* 的提交时间戳为 *si*。该不变性变为 *tabs(e commit 1 ) < tabs(e start 2 ) ⇒ s1 < s2*。执行事务和分配时间戳的协议服从两个规则，这两个规则会共同确保该不变性，详情下述。定义协调者 leader 的写事务 *Ti* 的提交请求到达事件为 *e server i*。

​		**Start** 协调者 leader 给写事务 *Ti* 分配一个不比 *TT.now().latest* 的值小的时间戳 *si*，该时间戳在 *e server i* 之后计算。注意参与者 leader 在这里并不相关；4.2.1 节将会描述它们是如何在第二条规则内引入的。
Commit Wait The coordinator leader ensures that clients cannot see any data committed by Ti until TT.after(si) is true. Commit wait ensures that si is less than the absolute commit time of Ti , or si < tabs(e commit i ). The implementation of commit wait is described in Section 4.2.1. Proof:

​		**Commit Wait** 协调者 leader 确保客户端在 *TT.after(si) * 为 true 之前不会看到任何由 *Ti* 提交的数据。提交会等待确保 *si* 小于 *Ti* 的绝对提交时间，或 *si < tabs(e commit i )*。Commit Wait 的实现描述在4.2.1节。证明：
```
s1 < tabs(e commit 1 ) (commit wait) tabs(e commit 1 ) < tabs(e start 2 ) (assumption) tabs(e start 2 ) ≤ tabs(e server 2 ) (causality) tabs(e server 2 ) ≤ s2 (start) s1 < s2 (transitivity)
```
