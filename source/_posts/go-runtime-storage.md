---
title: Go Runtime 设计：存储资源管理
mathjax: true
date: 2022-04-06 21:12:05
tags: 
- source
- go
categories:
- Golang
---

<img src="https://raw.githubusercontent.com/MariaLetta/free-gophers-pack/8f7fbe7906dd4433a5719df73d3dde6f481b459f/goroutines/svg/15.svg" width="500;" />

本文介绍了 Golang Runtime 中关于内存管理的设计。

<!-- more -->

内存管理，主要可以分为两部分：内存分配和内存回收。

在 Go 语言中，所有需要内存的地方，除了编译期可以确定的常量、变量值以外，其他的包括 goroutine 私有栈，堆以及一些 runtime 的结构，都需要在运行期间动态进行分配。相对应的，当内存不再使用的时候，也需要动态的回收。

不过 Go 语言的内存分配完全不需要用户操心，Runtime 会帮助用户处理好与内存管理相关的一切。

本文会先讨论 Go Runtime 使用的动态内存分配器；之后介绍所有需要动态分配内存的地方如栈、堆等等，具体是如何使用内存分配器分配内存的；关于内存的动态回收，将放在最后一部分讲解。



## 1. 动态内存分配器

### 1.1 为什么需要动态内存分配器？

在讨论动态内存分配之前，我们可以先回顾一下操作系统的内存管理，并解释为什么不能简单的使用系统调用来管理进程内存。



#### 虚拟内存抽象

为了能让不同的进程能优雅的共享同一套存储硬件，多任务操作系统普遍采用了地址空间 + 虚拟内存的办法，对物理内存进行了抽象。

<img src="https://upload.wikimedia.org/wikipedia/commons/6/6e/Virtual_memory.svg" style="zoom:50%;" />

这种抽象的本质是把物理内存视为磁盘的缓存，而把磁盘视为主存。

虚拟内存为多进程的并发执行带来了如下的好处：

- 通过地址映射（mapping）技术，可以支持多进程独占地址空间的同时还能共享同一套硬件
  - 每个进程都拥有自己的页表，用于记录进程虚拟地址与物理内存地址的映射关系
  - 进程可以自由的在整个地址空间中申请并使用内存，而不必担心与其他进程产生冲突

- 通过交换（swaping）技术，虚拟内存能够让进程可操作的地址空间远大于实际的物理内存
  - 访问页表中不存在的映射触发缺页中断，操作系统从磁盘（主存）中加载数据到物理内存（缓存）
  - 如果物理内存已满，则会通过页面置换算法将某些进程的某些页面淘汰到磁盘，而空出内存空间



#### 内存布局

为了方便对进程使用虚拟内存，操作系统还对整个进程的地址空间进行了分段，Linux 下进程的内存布局如下（图源：*CSAPP Figure 9.26*）：

{% asset_img mem-layout.jpg %}

Linux 进程的整个虚拟地址空间，从 0x400000 开始，到栈区高地址结束，更高的地址空间留给内核。

进程内存段从低地址开始分别被划分为：

- 代码段 `.text`
- 已初始化数据段 `.data`
- 未初始化数据段 `.bss`
- 运行时堆 `heap`
- 共享库内存映射区 `mmap`
- 运行时栈 `stack`

如上图所示，对于运行中的进程，如果对内存有需要时，可以有三种方式：

1. 向低地址扩张栈指针，等同于分配栈空间
2. 通过 brk 系统调用，向高地址扩张，分配堆空间
3. 通过 mmap 系统调用，在内存映射区分配空间

当然了，通过上述过程所分配的内存空间，在实际访问的那一刻之前，都没有实际分配，只有在访问时才会触发缺页中断。

既然操作系统已经提供了这么多种内存分配的系统调用，我们为什么还要使用用户层代码来动态的管理内存呢？



#### 动态内存管理

首先需要明确的是，我们所谓的动态内存管理，通常都是用来管理堆内存。而动态内存管理需要解决的本质问题是在运行时对内存需求的不确定性。

相反对于栈内存的分配和回收，通常在编译期就能明确栈空间所需的大小，因此就可以简单的通过操作 SP 指针来管理栈内存。

那么用 brk 或 mmap 就不能实现动态的分配和回收堆内存吗？

理论上讲，是完全可以的，毕竟 brk 和 mmap 本身就是进程申请向 os 申请内存时必须要涉及到的系统调用。

**但问题主要在于：**

很多时候，进程需要的是频繁的分配/释放小块空间，而不是少量的分配/释放大块空间。但由于执行系统调用所耗费的时间高于执行库函数，这就导致分配/释放小块空间的过程中，系统调用的耗时对总耗时产生了可观的影响。

那么，如果当需要分配小块空间时，一次性多分配一些，这样在下一次又需要分配小块空间时，就可以不用再通过系统调用。反之，当想要释放小块空间时，暂时不释放给系统，而是攒到一定程度后再合并释放，也能减少系统调用次数。

为了演示上述问题，我们来看如下测试，

执行一百万次给定 size 内存空间的申请、初始化、释放过程，耗时对比（越短说明越快）：

| Alloc Size | malloc/ms | brk/ms | mmap/ms |
| ---------- | --------- | ------ | ------- |
| 128 Bytes  | 393       | 4738   | 5235    |
| 1 KiB      | 2902      | 8791   | 8004    |
| 4 KiB      | 11348     | 17375  | 16321   |
| 16 KiB     | 44793     | 56489  | 56832   |

结果表明，不论是哪种 size，brk 和 mmap 耗时差别不大，而 malloc 都比这些系统调用要快。更进一步的，当分配 128 bytes 空间时，malloc 的性能表现时系统调用的约 10 倍，随着分配空间的不断加大，malloc 的性能优势逐步从 3 倍降低到 1.5 倍最后到 1.2 倍。

所以，通过动态内存管理，将零散的内存分配/释放请求合并起来，减少了频繁的系统调用，提升了性能。



### 1.2 不同的内存分配器算法

#### 1.2.1 设计要求、目标、障碍

上一节我们看到了动态内存分配器相比于系统调用的优势。那么，如果要设计一个内存分配器，我们对它有什么要求？设计目标又是什么？

在《CSAPP》§9.9.3 中对动态内存分配器的设计要求和设计目标做了阐述：

**设计要求**

- 处理任意请求序列：分配器不能假设分配和释放的顺序，用户可以任意的进行分配或释放
- 立即响应请求：不能为了提高性能而对分配请求进行缓存或重排（分配请求所返回的内存地址必须是立即可用的，相反释放请求不需要立即完成）
- 只使用堆：为了可扩展，分配器自己用到的所有非标量数据结构都要放在堆上
- 块对齐：分配的块必须对齐
- 不修改已分配的块：一旦块被分配出去，就不允许分配器对其进行任何的修改或移动（防止用户访问到非法区域），相反空闲块可以被修改或移动



**设计目标**

1. 最大化吞吐率：即单位时间内处理的请求量最大化。这要求我们尽可能的缩减分配/释放请求的耗时。简单的方式是让分配请求耗时与空闲块数量成线性反比，而释放请求耗时为常数。
2. 最大化内存利用率：内存资源是有限的，最大化利用率意味着最小的浪费。峰值利用率代表在 n 个分配/释放请求过程中，在某一刻分配出去的最大有效载荷之和，与堆空间容量的占比。占比越高说明对空间的利用率越高。

显然，最大化吞吐率和最大化内存利用率之间会相互牵制，我们无法设计出一个既满足最大化吞吐率又满足最大化利用率的分配器，而分配器的设计挑战就在于寻找平衡。



**设计障碍**

动态内存管理，在改善性能的同时，也会引入新的问题，如下的两种问题，会严重的影响内存分配器实际的性能表现：

- 碎片：

  应用程序对分配内存大小的要求，不是整齐划一的，经常会出现请求的容量忽大忽小，这就可能导致碎片的产生。

  {% asset_img fragment.jpg %}

  如上图所示，在总共 20 KiB 的可用空间里，红色的部分代表已分配，其他的是空闲，那么假设，

  1. 需要分配 8 KiB：

     需要寻找连续的 8 KiB 空闲区，那么假如从头开始搜索，就需要搜索到第 6 块处才能找到合适的空间。

     由于前面的小碎片空间，无法满足需求，因此会增加查找成本，降低性能。

  2. 需要分配 16KiB：

     从头开始搜索，一直到最后都无法找到 16KiB 的连续空闲区。但实际上剩余的空闲空间，正好是 16KiB。

  随着碎片的不断产生，如果处理不当，分配请求可能会越来越慢。引用[这篇文章](https://johnysswlab.com/the-price-of-dynamic-memory-allocation/)中举的例子：

  *”某款 TV 盒子的测试，是连续 24 小时间，每 10 秒钟换一次台。由于内存碎片的严重影响，刚开始在换台后 1 秒钟视频就能播放，而24 小时候，这一过程需要花费 7 秒。“*

- 竞争：

  动态内存管理程序必须考虑多线程竞争请求的问题。保证线程安全，最简单的办法当然是加锁，然而加锁是有不小的开销的。当分配器能很顺利的找到空闲区时，加锁解锁的开销可能会产生可观的耗时占比。

  要处理竞争问题，一个可行的办法就是尽量避免竞争。

  实际当中可以通过线程缓存等方式来确保只有单线程发起请求，然而，某些情况下 -- 比如内存在一个线程申请，在另一个线程释放 -- 就会产生额外开销。同时，线程缓存本身也会降低内存利用率。



#### 1.2.2 dlmalloc

dlmalloc 是 Doug Lea 设计的内存分配器，在早期被广泛使用，后来由于它在多线程下存在的一些问题，由 Wolfram Gloger 将之 fork 并进行优化后，改名为 ptmalloc，目前最新的 ptmalloc 是 ptmalloc3 (但最新的 glic 2.36 中仍然采用的是 ptmalloc2)。





### 1.3 Go 内存分配器设计



## 2. Go 的内存划分

### 2.1 进程内存

不论 Go 在 Runtime 中如何组织 goroutine 的内存布局，从操作系统的视角来看，加载一个 Go 可执行文件，与加载其他可执行文件没什么区别，最终都会以进程的形式运行在操作系统上。

因此在具体分析 Go 自己的内存布局和管理之前，我们先来回顾一下 Linux 的内存布局（图源：*CSAPP Figure 9.26*）：

{% asset_img mem-layout.jpg %}

Linux 进程的整个虚拟地址空间，从 0x400000 开始，到栈区高地址结束，更高的地址空间留给内核。

进程内存段从低地址开始分别被划分为：

- 代码段 `.text`
- 已初始化数据段 `.data`
- 未初始化数据段 `.bss`
- 运行时堆 `heap`
- 共享库内存映射区 `mmap`
- 运行时栈 `stack`

上述各分段的意义不再赘述，我们任意找到一个运行中的进程，通过访问 `/proc/{pid}/maps` 来看一看实际的内存布局：

```shell
[root@xxx ~]# pmap -X 1410
### 可以发现我们执行的是 sleep 1000
1410:   sleep 1000
         Address Perm   Offset Device  Inode   Size Rss Pss Referenced Anonymous Swap Locked Mapping
### .text
        00400000 r-xp 00000000  fd:01 789707     24  16  16         16         0    0      0 sleep
### .data
        00606000 r--p 00006000  fd:01 789707      4   4   4          4         4    0      0 sleep
### .bss
        00607000 rw-p 00007000  fd:01 789707      4   4   4          4         4    0      0 sleep
### heap，ptmalloc 堆初始空间为 132KiB
        01963000 rw-p 00000000  00:00      0    132  12  12         12        12    0      0 [heap]
### mmap，下同
    7f81ea7b0000 r--p 00000000  fd:01 801455 103692  48  10         48         0    0      0 locale-archive
    7f81f0cf3000 r-xp 00000000  fd:01 788272   1808 416  41        416         0    0      0 libc-2.17.so
    7f81f0eb7000 ---p 001c4000  fd:01 788272   2044   0   0          0         0    0      0 libc-2.17.so
    7f81f10b6000 r--p 001c3000  fd:01 788272     16  16  16         16        16    0      0 libc-2.17.so
    7f81f10ba000 rw-p 001c7000  fd:01 788272      8   8   8          8         8    0      0 libc-2.17.so
    7f81f10bc000 rw-p 00000000  00:00      0     20  12  12         12        12    0      0
    7f81f10c1000 r-xp 00000000  fd:01 786459    136 108   9        108         0    0      0 ld-2.17.so
    7f81f12d7000 rw-p 00000000  00:00      0     12  12  12         12        12    0      0
    7f81f12e1000 rw-p 00000000  00:00      0      4   4   4          4         4    0      0
    7f81f12e2000 r--p 00021000  fd:01 786459      4   4   4          4         4    0      0 ld-2.17.so
    7f81f12e3000 rw-p 00022000  fd:01 786459      4   4   4          4         4    0      0 ld-2.17.so
    7f81f12e4000 rw-p 00000000  00:00      0      4   4   4          4         4    0      0
### stack，默认容量 132KiB    
    7fff33464000 rw-p 00000000  00:00      0    132  12  12         12        12    0      0 [stack]
    7fff335a7000 r-xp 00000000  00:00      0      8   4   0          4         0    0      0 [vdso]
ffffffffff600000 r-xp 00000000  00:00      0      4   0   0          0         0    0      0 [vsyscall]
                                             ====== === === ========== ========= ==== ======
                                             108060 688 172        688        96    0      0 KB
```

前面是一个简单的 sleep 进程的内存布局，主要作为一个基准，用于和 Go 进程作比较。

现在来看一看一个 Go 进程的内存布局：

```shell
[root@xxx ~]# pmap -X 11564
### vmlet 是一个用 go 实现的 agent 程序，代码规模大约是 3-5k 行
11564:   /opt/vmlet/vmlet run -c ./vmlet.yaml
         Address Perm   Offset Device  Inode   Size   Rss   Pss Referenced Anonymous Swap Locked Mapping
### .text
        00400000 r-xp 00000000  fd:01 524358   6724  2272  2272       2272         0    0      0 vmlet
### .data        
        00a91000 r--p 00691000  fd:01 524358   7176  2644  2644       2620         0    0      0 vmlet
### .bss
        01193000 rw-p 00d93000  fd:01 524358    488   180   180        180       120    0      0 vmlet
### mmap，可以看到下面全部都是匿名的 mmap（没有 fd）      
        0120d000 rw-p 00000000  00:00      0    272   108   108        108       108    0      0
      c000000000 rw-p 00000000  00:00      0  81920  4252  4252       4200      4252    0      0
      c005000000 ---p 00000000  00:00      0  49152     0     0          0         0    0      0
    7f7eb7d55000 rw-p 00000000  00:00      0  40648  4884  4884       4884      4884    0      0
    7f7eba507000 ---p 00000000  00:00      0 263680     0     0          0         0    0      0
    7f7eca687000 rw-p 00000000  00:00      0      4     4     4          4         4    0      0
    7f7eca688000 ---p 00000000  00:00      0 293564     0     0          0         0    0      0
    7f7edc537000 rw-p 00000000  00:00      0      4     4     4          4         4    0      0
    7f7edc538000 ---p 00000000  00:00      0  36692     0     0          0         0    0      0
    7f7ede90d000 rw-p 00000000  00:00      0      4     4     4          4         4    0      0
    7f7ede90e000 ---p 00000000  00:00      0   4580     0     0          0         0    0      0
    7f7eded87000 rw-p 00000000  00:00      0      4     4     4          4         4    0      0
    7f7eded88000 ---p 00000000  00:00      0    508     0     0          0         0    0      0
    7f7edee07000 rw-p 00000000  00:00      0    384    40    40         40        40    0      0
### stack，默认容量 132KiB    
    7ffe4432c000 rw-p 00000000  00:00      0    132    16    16         16        16    0      0 [stack]
    7ffe44353000 r-xp 00000000  00:00      0      8     4     0          4         0    0      0 [vdso]
ffffffffff600000 r-xp 00000000  00:00      0      4     0     0          0         0    0      0 [vsyscall]
                                             ====== ===== ===== ========== ========= ==== ======
                                             785948 14416 14412      14340      9436    0      0 KB
```

从上面两个不同进程的内存布局中，我们能发现几个有趣的地方：

1. sleep 有`[heap]`，go 进程没有`[heap]`？
   - 对于`Mapping = [heap]`，只有当程序使用 `brk()` 分配堆内存后才会显示，以 `mmap()` 的形式分配的内存不显示为 `[heap]`
   - sleep 属于 glibc 库，glibc 默认的 ptmalloc 初始化时会通过 `brk()` 分配 132KiB 的堆空间
   - 后文会提到，go runtime 使用的内存分配器，是完全采用 `mmap()` 来分配内存的，没有调用过`brk()`，也就不显示 `[heap]`
2. 为什么 `[stack]` 的默认容量是 132KiB？
   - Linux 在执行 exec 时，初始化栈空间的逻辑中为栈顶[分配了 `PAGE_SIZE` 的空间](https://github.com/torvalds/linux/blob/1862a69c917417142190bc18c8ce16680598664b/fs/exec.c#L272)，amd64 架构下 linux page size 默认是 4KiB
   - 在之后的 [`setup_arg_pages()`](https://github.com/torvalds/linux/blob/1862a69c917417142190bc18c8ce16680598664b/fs/exec.c#L747) 中，对栈进行扩展，[扩展容量是硬编码的 128KiB](https://github.com/torvalds/linux/blob/1862a69c917417142190bc18c8ce16680598664b/fs/exec.c#L836)，4 + 128 = 132KiB，上面两个进程对栈的使用都没有超过 132KiB，所以 `[stack]` 都是 132KiB
   - 后文会提到，goroutine 的栈内存，也全都是从 `mmap()` 而来，只有主进程所在的 g0 栈直接使用系统栈，但 g0 限制了栈空间最大 8KiB，不会超出 132KiB

### 2.2 栈内存

在[本系列上一篇中](https://lenshood.github.io/2022/03/09/go-runtime-compute/)，我们已经知道，goroutine 持有自己的运行栈。

栈在创建 g 的时候需要一并分配出来，在销毁 g 的时候也需要一并清除掉。那么 go runtime 是从哪里为 g 分配栈内存的呢？

{% asset_img stack.jpg %}

上图展示了 g 的栈分配结构，在分配栈内存时，首先需要根据栈空间的大小来决定是从哪里分配。

1. 每个 P 都会持有一部分称为 `stack cache` 的空间，专门用于分配较小的栈
2. 假如需要分配的栈空间比较大，就会选择直接从全局的 `large stack pool`中进行分配

对于较小的栈，按照尺寸划分成 4 阶（linux 系统，其他系统阶数会有差异），每一阶的内存块分别指定为 2KiB、4KiB、8Kib、16KiB。对于小栈分配，根据所需空间大小向上取二次幂，然后从合适的阶中分配内存。

#### stack cache

由于小栈空间占比小，分配的频次也比较频繁（创建 g 默认栈2KiB，之后逐步扩容），因此在每一个 P 中都存放有一个本地的`stack cache`，从缓存中分配栈，不需要加锁，速度更快。

定义了 `stack cache` 每一阶最大不超过 32KiB，超出后会释放一半，剩 16KiB。而当分配时发现某一阶缓存为空，则会从全局的`stack pool` 中分配总量为 16KiB 的空间放入缓存。

#### stack pool

全局的小栈空间池。按照四个阶存储四条链表，用来保存固定大小的可用内存块。

当 `stack pool` 中可用空间耗尽后，会一次性从全局 heap 中申请 32KiB 的内存，并按阶切分成小块，插入链表。

#### large stack pool

从前面可知，小栈空间单个最大空间块是 16KiB，所以如果需要超过 16KiB 的栈，就需要从 `large stack pool` 中申请。

与小栈类似，存放大栈的`large stack pool` 也会分阶对内存块进行管理，只不过不再是按照 2 KiB 的幂次划分，而是按照 2 * Page 的幂次逐次递增。Go Runtime 定义一个 Page 的大小是 8192 Bytes，由于内存需求至少要 16 KiB 才会进入大栈分配，因此最少会分配 2 * Page。之后随着容量的增大，在 linux 下最高可以分配满 48bit 用户地址空间，即 2^35 个 Page。

对于大栈空间，当`large stack pool` 对应阶链表中不存在所需内存块时，也会先从全局 heap 分配 nPage 的空间，待栈释放时，将该空间加入 `large stack pool `中备用。



### 2.3 堆内存



### 2.4 其他内存



## 3. 垃圾收集器

### 3.1 不同的垃圾收集算法



### 3.2 Go 垃圾收集器设计
