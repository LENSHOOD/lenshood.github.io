---
title: Kubernetes 多集群的发展与趋势
date: 2023-02-07 20:33:21
tags:
- multi-cluster
- multi-cloud
- k8s
categories:
- Kubernetes
---

{% asset_img header.jpg 500 %}

本文介绍了 K8s 多集群的由来以及实现多集群所面临的核心问题，之后分析并探讨了现有的 K8s 多集群方案，最后根据目前实现方案的痛点与挑战，设想了未来的演进趋势。

<!-- more -->

## 为什么需要 K8s 多集群？

在探讨为什么需要 K8s 多集群之前，我们首先定义一下什么是 K8s 多集群：

所谓 K8s 多集群，顾名思义就是多个 K8s 集群，组织可根据自身需求，例如为了满足隔离性、可用性、合规性或使用成本等，将其应用程序运行在任意一个或多个集群中。此外，在更加成熟的 K8s 多集群中，应用程序实际运行的集群能够动态配置，不同集群间的应用程序也应该支持相互访问。

定义讲完，我们会发现，多集群的概念是被 K8s 限定的，那么问题来了：

### 在 K8s 之前，还存在多集群吗？

不论 K8s 是否存在，对应用的隔离性、可用性、合规性或使用成本等需求是持续存在的，因此我们总能听到企业购买或构建的所谓云管平台或称多云管理平台（架构如下图所示）。云管平台正是为了满足企业对应用部署的复杂需求，对接各种不同公/私有云的 API，并尝试对云资源进行抽象，进而尝试将应用运行所需的环境与真实云资源解耦。

![](http://cds.chinadaily.com.cn/dams/capital/image/202205/31/6295b81ae4b013b65bf3ff36.jpg)

然而，由于各种云提供商的 API 都不尽相同，云管平台的主要工作更多的是做 API 集成，并且每多一种云，就需要进行一次集成（Terraform Provider 的出现极大的缓解了这一问题）。随着企业逐步依赖在云管平台上交付应用，也就逐步依赖了云管平台的厂商。

回到现在。在 K8s 为云原生应用提供了统一交付体验的同时，也很大程度上消除了云管平台集成 API 的苦力活。如果企业没有太过复杂的动态调度需求，只需要在每个云上部署一套 K8s，应用就很容易通过原有的流水线在不同云的 K8s 上进行交付。

因此，**K8s 多集群的主要应用场景就是：多云适配。**

### 多集群的应用场景

虽然不同企业面临的问题不同，对应用交付的需求也不同，但总体来看可能存在以下几种多集群的常见应用场景：

**隔离性**：在多租户隔离，开发/测试/生产环境隔离，地区合规性隔离等场景下，相互隔离的应用部署在相互隔离的 K8s 集群中。

**高可用与故障转移**：为了保证应用的高可用性，当主集群出现严重故障时，集群上运行的应用能够迁移至备用集群。集群之间可以互为主备，并通过接入不同云厂商来降低风险。

**单集群规模**：在某些大规模场景下，单集群节点数逐步到达上限，集群控制面性能成为瓶颈，将大集群拆分为小集群可以缓解集群性能问题。

**弹性突发**：将集群也视为弹性资源，在非业务高峰期，应用运行在成本更低的私有云上，一旦出现业务高峰，私有云资源不足时，可以快速在公有云创建集群并调度应用副本，高峰过后再销毁。

**地理亲和性**：顾名思义，应用部署在多个集群上，而不同集群处于不同的地理位置，这样就能将用户请求根据实际发出的位置，路由到距离最近的应用上。

总结上述场景，落地 K8s 多集群，不外乎是通过赋予业务应用更灵活的弹性，而实现业务的安全性、高可用性与可迁移性，若能叠加集群本身的弹性，则可以真正做到云原生的所谓 ”无限资源“。



## 实现多集群管理的核心问题

上一节介绍了企业对多集群需求的现实性，假如企业已经决定开始尝试采纳多集群技术，那么势必会面临多集群的管理问题。

对于多集群的管理，存在哪些现实问题需要解决呢？这一节我们就来讨论实现多集群管理需要解决的核心问题。

### 多集群部署模型

企业落地多集群时，最先要考虑的就是：多集群的部署架构，其整体上是什么样子的？

#### 控制面模型

想要对多集群进行管理，需要有特定的管理软件来负责集群的加入/逐出，状态管理以及应用调度等，这种管理软件，我们可以称之为多集群的控制面。相应的，实际运行应用的 K8s 集群，我们称之为多集群的数据面。

控制面与数据面是逻辑上的概念，在实际的部署层面看，可以有如下两种情形：

1. 控制面独占集群资源

{% asset_img deploy-arch-independent-control.jpg %}

如图所示，多集群管理相关的控制面组件，独占整个集群作为 “控制集群”，存在多个控制集群确保控制面高可用。数据面则包含了数个 “工作集群”，每个工作集群内只运行实际的业务应用，所有的多集群管理决策，都在控制集群进行，实际的操作则通过控制集群连接到工作集群来进行。

这种部署架构下，控制面与数据面物理隔离，相互之间影响很小，但控制集群也占用了相对较多的资源，这种架构适用于复杂的，对隔离性、稳定性要求高的大型多集群。

2. 控制面与数据面共享集群资源

{% asset_img deploy-arch-hybrid.jpg %}

上图所表述的是另一种部署架构，集群之间不区分控制集群还是工作集群，都视为通用集群。控制面组件与业务应用都部署在同一个通用集群内，控制面组件之间通过选举机制来选定主从，主控制面在某个通用集群上发出对所有通用集群的管理决策。

这种部署架构下，控制面与数据面之间存在相互影响的可能，但总体资源成本更低。这种架构适用于小型多集群。

#### 网络模型

实际的场景下，多集群之间不论是否都是基于同一家公/私有云搭建，集群与集群之间应该都会存在网络隔离，各自在各自的子网内工作。那么在多集群协作的过程中，网络连通性就成为一大需要解决的问题。

集群之间的网络到底是连通还是隔断，与使用场景高度相关。如果是租户隔离场景，不同租户的集群资源之间网络隔离比逻辑隔离更安全，而在高可用或是弹性突发等场景下，由于业务应用之间存在东西向网路访问，就必须要确保不论应用运行在哪里，互相都能访问通。另外，多集群控制面必须要能和所有集群连通，否则根本无法实施管理决策。

集群之间的互联互通一半可以通过如下两种方式来实现：

1. 网关路由

   {% asset_img network-gateway.jpg %}

   如图所示，网关路由的形态很简单，首先通过在集群中安装网关来打通集群内外，之后任何涉及对其他集群的访问，网关都应当了解访问目的集群的地址（网关地址），并路由数据包至目的集群。

   网关路由的模型非常类似于 [istio 的多网络模型](https://istio.io/latest/docs/ops/deployment/deployment-models/#multiple-networks)，跨集群业务应用间的网络活动被网关转发代理实现通信，由于跨集群网络其 IP 地址可能存在冲突，因此需要通过合理的服务发现和治理机制确保服务名的唯一性。

2. Overlay 网络

   {% asset_img network-overlay.jpg %}

   我们知道很多 CNI 的工作模式就是在集群内构建了 Overlay 网络，因此如果能对单集群的 Overlay 网络进行扩展，使得多个集群之间共享同一个子网，也就实现了集群间整体的互联互通。

   

#### 服务发现与治理



### 跨集群应用调度

多集群的本质上是为了让应用运行在更合适的位置。应用到底运行在哪里，要考虑的决策点可以有很多，例如高可用、成本、合规性、地理位置等等。因此多集群管理中首先要解决的问题就是，跨集群应用该如何灵活的进行调度。

实现跨集群调度的方式有很多，最简单的，可以将不同应用需要运行的位置提前定好，随着 CI/CD 流水线的发布，应用自然而然就运行好了。当然了这种调度策略属于一种全静态的人工调度，能用但不够灵活。

就像 K8s 一样，自动化且灵活的调度，依赖运行在集群内的调度器软件，多集群管理也需要考虑设计自动化的调度器。

如下图所示，不同领域的调度器，其调度模型实际上是高度一致的：

{% asset_img logical-sched.png %}

调度的过程就是根据一定规则为任务分配资源。这里涉及到三个概念：任务，资源，规则。在多集群管理的语境下，任务就是应用，资源就是集群，而规则是一系列能够影响调度器决策的的调度策略。应用与集群属于既有系统中的概念，因此设计跨集群应用调度器，调度策略是核心。

#### 调度策略

通过调度策略，我们期望能解决 ”什么样的应用” 需要被调度到 “哪类集群” 的问题。显然，应用有其自身独特的属性集，集群也一样。从属性集的角度看，调度策略问题就可以转化为应用与集群属性集之间的最优匹配问题。

{% asset_img sched-chain.jpg. %}

举例说明，应用的属性集可能包括：命名空间、资源依赖、副本数、镜像名、租户归属、应用亲和性/反亲和性、最小资源需求等等，集群的属性集可能包括 AZ、地区（Region）、节点数、已分配 Pod 数、资源总量/余量、污点（Taint）等等。

基于上述属性集的的匹配策略可能包括：

- 应用的命名空间必须与集群一致
- 应用需要与亲和性应用绑定在同一个集群
- 有污点的集群不接受新应用
- 多个集群满足要求则平均分配副本
- 部署偏好尽可能平衡集群间的资源
- 高优先级的应用可以抢占低优先级的应用
- 应用存在依赖如 Secrets、Volume 等需要优先调度依赖

借鉴 K8s 的调度实现模型，上述每一种匹配策略都可作为一种决策器，通过检查它所关注的属性来做出调度决策。而决策器又可分为过滤型和打分型，过滤型决策器给出成功或失败的决策结果，打分型决策器给出分数。

进行调度决策时，待调度应用与待选集群的属性集依次通过所有过滤型决策器和打分型决策器，最终找到一个（或一组，考虑多副本高可用）分数最高的集群，调度完成。

### 应用模型扩展

传统单集群场景下，应用的定义通常会直接使用 K8s 的原生 API（例如通过 helm 定义的应用包），也正因为 K8s 原生 API 的通用性，从侧面造就了 K8s 成为事实上的行业标准。通过前面两小节，我们了解到在多集群的场景下，对应用的控制与调度以及应用间互联互通都提出了更高的要求。同样的，为了满足叠加多集群管理需求，传统 K8s 应用的模型也需要进行扩展。

扩展的应用模型设计，有如下三点考量：

#### 规格扩展与状态扩展

应用规格扩展的就是应用本身的属性。

根据前文我们了解到，应用调度策略会将与跨集群相关的应用属性纳入决策，典型的例子就包括应用与应用/地域之间的亲和性或反亲和性，以及不同业务关键性的应用对资源成本的优先级等等。而在跨集群网络连通性上，应用是否依赖跨集群的其他应用，以及应用自身是否暴露给其他集群等等，也是重要的扩展属性。

规格的扩展可分为两类：限制（constrains）和提示（hints）。

- 限制（constrains）：代表了应用对跨集群管理的强制性要求，如亲和性/反亲和性，最小副本数，污点容忍性（Taints Tolerations）等等
- 提示（hints）：代表了对多集群管理决策与动作的非强制性提示，如优先级，副本分配偏好，资源需求等等

状态扩展主要扩展的是应用在多个集群上的状态。这包括应用实际在每个集群上的副本数，运行健康状况，曾经被调度的历史等等。

#### 前向兼容性

在应用多集群之前，企业通常已经在单集群上拥有了成规模的稳定业务，因此在设计多集群应用模型时，一个重要的考量点就是如何更简单的兼容并迁移原有业务。

假如多集群的应用模型相比传统 K8s API 是破坏性的，这将导致需要修改现存的应用定义才能过渡到多集群下，这种对稳定业务的迁移流程，其成本和风险都是较大的。

因此，多集群应用模型的设计，应该尽量避免修改原有的应用定义，前文提到的规格与状态扩展，都通过独立的 API 进行管理，并通过合理的 Selector 与原始应用进行绑定。

#### 自定义资源对象

现代的 K8s 集群中通常都会运行着大量自定义资源对象来实现各种基础设施层的功能。多集群中对自定义资源对象的模型扩展，可以考虑如下两点要求：

1. 自定义资源对象的调度分发应该与原生资源对象保持一致
   - CRD 被注册后，可以根据策略自动分发到其他集群
   - CR 可以被多集群管理软件识别，并按需进行调度

2. 由于自定义资源对象灵活的结构，需要有方法获取到自定义资源对象的规格和状态来实施调度
   - 通过 “惯例” 获取 CR 的规格与状态（如资源要求，副本数，健康度等）
   - 通过用户扩展插件更精准的获取 CR 的规格和状态

### 集群即资源

K8s 为应用层提供了标准的容器编排调度能力，而在基础设施层提供了对节点资源的纳管能力，因此随着应用层需求的波动，节点资源也可以灵活的弹性扩缩。

成熟的多集群管理环境，弹性扩缩的不只是节点，集群本身也可以作为一种资源灵活的扩缩容。结合前面提到的多集群调度、网络连通和应用资源模型，一旦实现了集群即资源，不论是在业务拓展、合法合规还是成本优化等场景，都可以快速的完成多集群部署与回收，集群即资源是复杂多集群调度的自动化基础。

#### 集群生命周期

既然集群也作为一种资源，那么集群的生命周期也就可以基于 K8s 风格的 API 来进行操作和管理。

例如定义了一个类型为 `WorkerCluster` 的资源对象以代表一个集群资源（见如下代码段），其中定义了该集群的属性，例如云厂商名，控制面节点数，数据面节点数等等。这时集群即资源的相关控制器就应该基于该描述在对应的云厂商中创建出一个集群。

```yaml
apiVersion: multi-cluster.demo.io/v1
kind: WorkerCluster
metadata:
  name: demo-worker-cluster
spec:
  cloudProvider: "aws",
  controlPlaneNodes: 3,
  dataPlaneNodes: 20,
  ... ...
```

而当上述资源描述发生变化（如对数据面节点进行了扩容）时，实际的集群也应该发生变化。显然，如果上述资源被删除，那么对应的集群也就需要被销毁掉。

通过上述例子我们可以发现，集群生命周期的管理，在兼容不同的云时，存在如下难点：不同云提供商、以及各种私有云方案，其基础设施操作 API 都不同，并且除了 K8s 自管方案，公有云都存在代管方案，如 AKS，GKE 等，创建集群时需要区分。

[ClusterAPI](https://github.com/kubernetes-sigs/cluster-api) 是 K8s “Cluster Lifecycle SIG（特别兴趣小组）” 发起的项目，Cluster API 尝试通过定义标准基础设施 API 来统一集群生命周期管理，各类云厂商自行提供实现了标准 API 的 “Provider” 来支持自动创建集群资源，由于其官方背景，目前已有[数十种 Provider](https://cluster-api.sigs.k8s.io/reference/providers.html) 可供选择（不仅包含 aws 等公有云，还包含了 OpenStack，OCI 等其他方案）。

也许未来 Cluster API 可能会成为集群声明周期管理的标准。

#### 集群资源模型

对集群进行扩缩容的前提是调度器能够准确的获悉集群当前的资源分配情况，不同的集群可能拥有完全不同的资源，其分配偏好差异也很大（例如云上的集群与边缘集群），因此需要需要一种通用的模型来描述集群自身的资源，从而指导调度决策。

资源模型中除了传统的 cpu、memory 外，需要支持多样的自定义资源，如 gpu，storage，ip 配额等等，另外资源模型不能只上报汇总的资源情况，还需考虑可能存在的资源碎片问题。

如下是一种可能的资源模型定义：

```yaml
apiVersion: multi-cluster.demo.io/v1
kind: WorkerCluster
metadata:
  name: demo-worker-cluster
... ...
status:
	resource:
    total:
      cpu: "20"
      memory: "100GiB"
      gpu: "10"
      ipPool: "127"
    available:
      cpu: "3.5"
      memory: "1200m"
      gpu: "2"
      ipPool: "10"
    continous:
      cpu: "2"
      memory: "550m"
      gpu: "1"
      ipPool: "10"
```

## 几种方案对比

### Karmada

### OCM

### Rancher

### KubeFed V2

## 演进趋势

1. 真的需要扁平网络吗？（跨集群 pod 同一个网络）
2. 跨集群方案怎么解决数据同步问题
2. 如何统一管理传统云资源，如 VM，块存储，VPC 等
